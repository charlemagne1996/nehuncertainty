{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting and normalizing metadata\n",
    "\n",
    "From June through September, 2020, we manually checked genre for about a thousand volumes identified as English-language fiction and biography. But the genres were often misidentified by the algorithmic process that selected these titles, so bio and fic ended up jumbled together in the same files.\n",
    "\n",
    "This Jupyter notebook assembles that data and sorts it into separate data frames for fiction and biography. It also \"normalizes\" the metadata. Human coders may use many slightly different versions of the same label, but when the data is ultimately used by an algorithm, we're going to need those labels to all be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, glob\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fictionmeta/supplementary_peizhen_fic.tsv',\n",
       " 'fictionmeta/peizhenfic_summer.tsv',\n",
       " 'fictionmeta/current_wenyi_fic.tsv',\n",
       " 'fictionmeta/current_morgan_fic.tsv',\n",
       " 'fictionmeta/allmatchedbiographies.tsv',\n",
       " 'fictionmeta/current_ted_fic.tsv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sourcefiles = glob.glob('fictionmeta/*.tsv')\n",
    "sourcefiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the rows\n",
    "\n",
    "We go through each file, identify it as bio or fic, and correct a few details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple field:  y\n",
      "multiple field:  nan\n",
      "multiple field:  nan\n",
      "multiple field:  nan\n",
      "multiple field:  nan\n",
      "njp.32101066714179|njp.32101066714187|njp.32101066714195\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "multiplectr = 0\n",
    "noctr = 0\n",
    "\n",
    "biorows = []\n",
    "ficrows = []\n",
    "weirdos = []\n",
    "norows = []\n",
    "\n",
    "weird_audiences = set()\n",
    "\n",
    "for filename in sourcefiles:\n",
    "    data = pd.read_csv(filename, sep = '\\t')\n",
    "    for idx, row in data.iterrows():\n",
    "        \n",
    "        # let's add a column for coder in case we need it later\n",
    "        \n",
    "        if 'peizhen' in filename:\n",
    "            row['coder'] = 'peizhen'\n",
    "        elif 'wenyi' in filename:\n",
    "            row['coder'] = 'wenyi'\n",
    "        elif 'morgan' in filename:\n",
    "            row['coder'] = 'morgan'\n",
    "        elif 'biographies' in filename:\n",
    "            row['coder'] = 'morgan'  # most likely\n",
    "        elif 'ted' in filename:\n",
    "            row['coder'] = 'ted'\n",
    "        \n",
    "        # The next few lines standardize our treatment of vols\n",
    "        # where there are multiple Hathi IDs aligned with a single\n",
    "        # Gutenberg ID. In that situation, we're going to have\n",
    "        # 'y' in the multiplehtids column, and the IDs themselves,\n",
    "        # in order and separated by pipes, in the docid column.\n",
    "        \n",
    "        if '|' in row['docid']:\n",
    "            print(\"multiple field: \", row['multiplehtids'])\n",
    "            row['multiplehtids'] = 'y'\n",
    "            multiplectr += 1\n",
    "        elif not pd.isnull(row['multiplehtids']) and '|' in row['multiplehtids']:\n",
    "            row['docid'] = row['multiplehtids']\n",
    "            row['multiplehtids'] = 'y'\n",
    "            print(row['docid'])\n",
    "            multiplectr += 1\n",
    "    \n",
    "        # the audience field should either contain nothing,\n",
    "        # or the tag 'juv.' There are lots of other things in there in\n",
    "        # practice, but I'm going to assume they are all equivalent to\n",
    "        # 'juv'. To ensure I'm not making a mistake, I'll print them\n",
    "        \n",
    "        if 'audience' not in row:\n",
    "            row['audience'] = float('nan')\n",
    "            \n",
    "        if (not pd.isnull(row['audience'])) and (not row['audience'] == 'juv'):\n",
    "            weird_audiences.add(row['audience'])\n",
    "            row['audience'] = 'juv'\n",
    "                  \n",
    "        # the following rows just sort bio from fic\n",
    "        # they assume that the genre string will begin with\n",
    "        # either 'bio' or 'fic' (or 'no')\n",
    "        # some special else-if statements deal with\n",
    "        # exceptions and normalize them\n",
    "        \n",
    "        genre = row['genre']\n",
    "        if pd.isnull(genre):\n",
    "            continue\n",
    "        elif genre.startswith('no'):\n",
    "            noctr += 1\n",
    "            norows.append(row)\n",
    "        elif genre.startswith('bio'):\n",
    "            biorows.append(row)\n",
    "        elif genre.startswith('fic'):\n",
    "            ficrows.append(row)\n",
    "        elif genre == 'historical fic':\n",
    "            row['genre'] = 'fic|historical'\n",
    "            ficrows.append(row)\n",
    "        elif genre == 'romance':\n",
    "            row['genre'] = 'fic|romance'\n",
    "            ficrows.append(row)\n",
    "        elif genre == 'adventure fic':\n",
    "            row['genre'] = 'fic'\n",
    "            ficrows.append(row)\n",
    "        else:\n",
    "            # what even is this?\n",
    "            weirdos.append(genre)\n",
    "            print(filename)\n",
    "\n",
    "biodf = pd.DataFrame(biorows)\n",
    "ficdf = pd.DataFrame(ficrows)\n",
    "nodf = pd.DataFrame(norows)\n",
    "            \n",
    "print(multiplectr)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Juvenile audience', 'children', 'juvenile', 'young adults'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird_audiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes we were right to assume that all weird strings in the audience field should be 'juv.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 22)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biodf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(564, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ficdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating genre\n",
    "\n",
    "Let's peer into those genre categories a little deeper and normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMMON GENRES:\n",
      "fic 564\n",
      "historical 95\n",
      "short 57\n",
      "romance 34\n",
      "domestic 33\n",
      "mystery 30\n",
      "adventure 20\n",
      "folklore 19\n",
      "travel 17\n",
      "western 6\n",
      "historical fic 6\n",
      "children 5\n",
      "fantasy 5\n",
      "juvenile 5\n",
      "historical fiction 4\n",
      "detective 4\n",
      "social 3\n",
      "female 2\n",
      "religious 2\n",
      "Short stories 2\n",
      "Fairy tales 2\n",
      "social life 2\n",
      "sf 2\n",
      "war 1\n",
      "narratives 1\n",
      "historical novel 1\n",
      "science fiction 1\n",
      "gothic 1\n",
      "poetry 1\n",
      "nature 1\n",
      "narrative 1\n",
      "music 1\n",
      "realistic 1\n",
      "myth 1\n",
      "international 1\n",
      "decadant 1\n"
     ]
    }
   ],
   "source": [
    "fiction_genres = ficdf['genre']\n",
    "\n",
    "genrectr = Counter()\n",
    "\n",
    "for genre in fiction_genres:\n",
    "    gparts = set(genre.split('|'))\n",
    "    for g in gparts:\n",
    "        g = g.strip()\n",
    "        genrectr[g] += 1\n",
    "\n",
    "print()\n",
    "print(\"COMMON GENRES:\")\n",
    "for g, count in genrectr.most_common():\n",
    "    print(g, count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only genres we officially planned to code were mystery, sf (science fiction), romance, short, folklore, and historical. Let's normalize to those. Other categories are too small to be useful for our present purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "official = {'mystery', 'historical', 'short', 'romance', 'mystery', 'folklore', 'sf'}\n",
    "ficdf = ficdf.reset_index()\n",
    "ficdf = ficdf.drop(axis = 1, labels = 'index')\n",
    "\n",
    "for idx in ficdf.index:\n",
    "    genre = ficdf.at[idx, 'genre']\n",
    "    gparts = [x.strip() for x in genre.split('|')]\n",
    "    officialgenres = set()\n",
    "    for g in gparts:\n",
    "        if g in official:\n",
    "            officialgenres.add(g)\n",
    "        elif g == 'science fiction':\n",
    "            officialgenres.add('sf')\n",
    "        elif g == 'historical fic':\n",
    "            officialgenres.add('historical')\n",
    "        elif g == 'historical novel':\n",
    "            officialgenres.add('historical')\n",
    "        elif g == 'Short stories':\n",
    "            officialgenres.add('short')\n",
    "        elif g == 'historical fiction':\n",
    "            officialgenres.add('historical')\n",
    "        elif g == 'detective':\n",
    "            officialgenres.add('mystery')\n",
    "        elif g == 'juvenile':\n",
    "            ficdf.at[idx, 'audience'] = 'juv'\n",
    "        elif g == 'children':\n",
    "            ficdf.at[idx, 'audience'] = 'juv'\n",
    "        else:\n",
    "            pass  # we don't add genres that aren't in this list\n",
    "        \n",
    "    if len(officialgenres) > 0:\n",
    "        normalizedgenrestring = 'fic | ' + ' | '.join(officialgenres)\n",
    "    else:\n",
    "        normalizedgenrestring = 'fic'\n",
    "    \n",
    "    ficdf.at[idx, 'genre'] = normalizedgenrestring\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ficdf = ficdf.sort_values(by = 'latestcomp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ficdf.to_csv('annotatednormalizedfictionmeta.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "biodf = biodf.sort_values(by = 'latestcomp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "biodf.to_csv('annotatednormalizedbiometa.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many books for young readers do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ficdf['audience'] == 'juv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the approximate distribution across the timeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ficdf['latestcomp'], bins = 50, range = (1700, 1960))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we don't have enough before 1820; we'll probably need to do a manual dive to find works of fiction in Hathi before 1820 that have Gutenberg (or ECCO) clean text versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMMON GENRES:\n",
      "fic 564\n",
      "historical 106\n",
      "short 59\n",
      "mystery 34\n",
      "romance 34\n",
      "folklore 19\n",
      "sf 3\n"
     ]
    }
   ],
   "source": [
    "fiction_genres = ficdf['genre']\n",
    "\n",
    "genrectr = Counter()\n",
    "\n",
    "for genre in fiction_genres:\n",
    "    gparts = set(genre.split('|'))\n",
    "    for g in gparts:\n",
    "        g = g.strip()\n",
    "        genrectr[g] += 1\n",
    "\n",
    "print()\n",
    "print(\"COMMON GENRES:\")\n",
    "for g, count in genrectr.most_common():\n",
    "    print(g, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fictionmeta/peizhenfic_summer.tsv', sep = '\\t')\n",
    "dups = df.pivot_table(index = ['gbindex'], aggfunc ='size') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = dups.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gbindex\n",
       "28346    3\n",
       "59236    1\n",
       "35997    1\n",
       "37243    1\n",
       "37180    1\n",
       "37118    1\n",
       "37046    1\n",
       "37029    1\n",
       "36953    1\n",
       "36914    1\n",
       "36758    1\n",
       "36626    1\n",
       "36612    1\n",
       "36602    1\n",
       "36414    1\n",
       "36160    1\n",
       "35805    1\n",
       "37415    1\n",
       "35717    1\n",
       "35507    1\n",
       "35500    1\n",
       "35119    1\n",
       "34996    1\n",
       "34939    1\n",
       "34883    1\n",
       "34852    1\n",
       "34830    1\n",
       "34811    1\n",
       "34801    1\n",
       "34297    1\n",
       "37250    1\n",
       "37453    1\n",
       "34009    1\n",
       "37627    1\n",
       "41256    1\n",
       "41182    1\n",
       "40893    1\n",
       "40874    1\n",
       "40735    1\n",
       "40726    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dups[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodf.to_csv('titles_rejected.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
