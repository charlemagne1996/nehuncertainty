{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fewer-bosnia",
   "metadata": {},
   "source": [
    "# A template for experiments\n",
    "\n",
    "I'm trying to develop a model we can use for experiments on the NEH data. But this is by no means set in stone yet; it's a first draft we should discuss and adjust.\n",
    "\n",
    "For a first test, let's consider the problem of author gender. We know our model of gender is imperfect, and we don't imagine a predictive model trained on this boundary will tell us very much about gender directly; it's almost certainly, to some degree, a proxy for genre. But it's a tricky boundary to model and thus a good place to start. We're in no danger of getting 100% accuracy!\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dried-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-integral",
   "metadata": {},
   "source": [
    "### Title metadata\n",
    "\n",
    "Eventually we will have detailed metadata for each \"chunk\" of a title, reporting things like error levels, the ratio between the lengths of clean-text and ocr-text chunks, etc.\n",
    "\n",
    "Right now I haven't created that yet. So our strategy for getting metadata will be to import title-level metadata and then map it out to the chunks we observe, using the heuristic that each chunk is named according to the formula ```gbindex_chunknumber.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "animal-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('../metadata/cleanrowswithhathimatches.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "noted-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanfiles = [x for x in os.listdir('/Users/tunder/Box Sync/NEHproject/cleannarratives/')\n",
    "              if x.endswith('.txt')]\n",
    "dirtyfiles = [x for x in os.listdir('/Users/tunder/Box Sync/NEHproject/dirtynarratives/')\n",
    "               if x.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "interesting-drove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 696 separate chunks.\n"
     ]
    }
   ],
   "source": [
    "assert len(cleanfiles) == len(dirtyfiles)\n",
    "print(\"We have \" + str(len(cleanfiles)) + \" separate chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "liquid-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'36965_3.txt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanfiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "north-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gbindex(filename):\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "gbdict = dict()\n",
    "\n",
    "for filename in cleanfiles:\n",
    "    gbindex = get_gbindex(filename)\n",
    "    if gbindex not in gbdict:\n",
    "        gbdict[gbindex] = []\n",
    "    gbdict[gbindex].append(filename)\n",
    "\n",
    "gbset = set(gbdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "central-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But only 121 separate titles.\n"
     ]
    }
   ],
   "source": [
    "print(\"But only \" + str(len(gbset)) + \" separate titles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "wooden-richards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 24)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ourtitles = metadata.loc[metadata['gbindex'].isin(gbset), : ]\n",
    "ourtitles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-treasurer",
   "metadata": {},
   "source": [
    "Lol, contrary to my assertion that ```cleanrowswithhathimatches``` has one row for each gbindex, there appear to still be duplicated gbindexes. I will need to fix that in the original. For right now, a kludge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "spatial-graphics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 24)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ourtitles = ourtitles.drop_duplicates(subset = 'gbindex')\n",
    "ourtitles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-upset",
   "metadata": {},
   "source": [
    "### Chunk metadata\n",
    "\n",
    "Eventually we will read this in directly. For  right now, a kludge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "capital-singer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>author</th>\n",
       "      <th>authordate</th>\n",
       "      <th>title</th>\n",
       "      <th>latestcomp</th>\n",
       "      <th>hathidate</th>\n",
       "      <th>imprint</th>\n",
       "      <th>gutenstring</th>\n",
       "      <th>enumcron</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>...</th>\n",
       "      <th>instances</th>\n",
       "      <th>genre</th>\n",
       "      <th>audience</th>\n",
       "      <th>authgender</th>\n",
       "      <th>multiplehtids</th>\n",
       "      <th>comments</th>\n",
       "      <th>coder</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Trimmed</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>27553_5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>27553_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>27553_6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>27553_7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "      <td>27553_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docid           author authordate              title  \\\n",
       "0  loc.ark+=13960=t5p851b8s  Reid, Stuart J.        NaN  Lord John Russell   \n",
       "0  loc.ark+=13960=t5p851b8s  Reid, Stuart J.        NaN  Lord John Russell   \n",
       "0  loc.ark+=13960=t5p851b8s  Reid, Stuart J.        NaN  Lord John Russell   \n",
       "0  loc.ark+=13960=t5p851b8s  Reid, Stuart J.        NaN  Lord John Russell   \n",
       "0  loc.ark+=13960=t5p851b8s  Reid, Stuart J.        NaN  Lord John Russell   \n",
       "\n",
       "   latestcomp  hathidate                       imprint  \\\n",
       "0        1895        NaN  New York;Harper & brothers;1   \n",
       "0        1895        NaN  New York;Harper & brothers;1   \n",
       "0        1895        NaN  New York;Harper & brothers;1   \n",
       "0        1895        NaN  New York;Harper & brothers;1   \n",
       "0        1895        NaN  New York;Harper & brothers;1   \n",
       "\n",
       "                           gutenstring enumcron gbindex  ...  instances  \\\n",
       "0  Reid, Stuart J. | Lord John Russell  <blank>   27553  ...        NaN   \n",
       "0  Reid, Stuart J. | Lord John Russell  <blank>   27553  ...        NaN   \n",
       "0  Reid, Stuart J. | Lord John Russell  <blank>   27553  ...        NaN   \n",
       "0  Reid, Stuart J. | Lord John Russell  <blank>   27553  ...        NaN   \n",
       "0  Reid, Stuart J. | Lord John Russell  <blank>   27553  ...        NaN   \n",
       "\n",
       "   genre audience authgender multiplehtids  comments   coder           Folder  \\\n",
       "0    bio      NaN          u           NaN       NaN  morgan  gutenbiotrimmed   \n",
       "0    bio      NaN          u           NaN       NaN  morgan  gutenbiotrimmed   \n",
       "0    bio      NaN          u           NaN       NaN  morgan  gutenbiotrimmed   \n",
       "0    bio      NaN          u           NaN       NaN  morgan  gutenbiotrimmed   \n",
       "0    bio      NaN          u           NaN       NaN  morgan  gutenbiotrimmed   \n",
       "\n",
       "   Trimmed     filename  \n",
       "0  Trimmed  27553_5.txt  \n",
       "0  Trimmed  27553_4.txt  \n",
       "0  Trimmed  27553_6.txt  \n",
       "0  Trimmed  27553_7.txt  \n",
       "0  Trimmed  27553_3.txt  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkmeta = []\n",
    "\n",
    "for idx, row in ourtitles.iterrows():\n",
    "    \n",
    "    gbindex = row['gbindex']\n",
    "    files4thisindex = gbdict[gbindex]\n",
    "    \n",
    "    for filename in files4thisindex:\n",
    "        chunkrow = pd.Series(row)\n",
    "        chunkrow['filename'] = filename\n",
    "        chunkmeta.append(chunkrow)\n",
    "\n",
    "chunkmeta = pd.DataFrame(chunkmeta)\n",
    "chunkmeta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-provision",
   "metadata": {},
   "source": [
    "### Analysis plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-range",
   "metadata": {},
   "source": [
    "We need to optimize the number of features and the regularization constant. But we don't want to overfit a particular sample. (There is in reality not a huge danger of overfitting with two parameters, but since we're trying to determine the risk of distortion as precisely as possible, it's best to be scrupulous here.)\n",
    "\n",
    "So I would propose that we separate our test set (1/4 of the data) from 3/4 of the data that we use for training-and-validation. That's to say, we cross-validate and optimize on 3/4 of the data (the same 3/4 for both clean and dirty corpora). And then finally test the model produced by that 3/4 on a held-out 1/4 of the data.\n",
    "\n",
    "The one additional complication is that we need to be dividing the data *by author.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "hawaiian-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(chunkmeta['author']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "continuous-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbott, Henry\n",
      "Abbott, Jacob,\n",
      "Alcott, Louisa May,\n",
      "Allen, George Hoyt\n",
      "Allen, Grant,\n",
      "Barine, Arvède,\n",
      "Barrie, J. M.\n",
      "Bates, Arlo\n",
      "Beerbohm, Julius\n",
      "Bell, Henry Glassford,\n",
      "Bettany, George Thomas,\n",
      "Blind, Mathilde,\n",
      "Blunden, Edmund\n",
      "Broun, Heywood\n",
      "Brown, E. E.\n",
      "Burney, Fanny\n",
      "Campan, Mme\n",
      "Carter, Harry\n",
      "Casson, Herbert Newton,\n",
      "Castlemon, Harry\n",
      "Cibber, Colley\n",
      "Colmache, Édouard\n",
      "Coolidge, Susan\n",
      "Cooper, James Fenimore\n",
      "Crockett, David\n",
      "Cruttwell, Maud.\n",
      "Daniels, Mabel W.\n",
      "Duffy, Bella\n",
      "Duncan, Norman\n",
      "Farrar, Geraldine.\n",
      "Gallishaw, John\n",
      "Gaskell, Elizabeth Cleghorn\n",
      "Gay, Sydney Howard,\n",
      "Giberne, Agnes\n",
      "Gilchrist, Anne Burrows,\n",
      "Goodwin, William\n",
      "Graham, Isabella\n",
      "Grimaldi, Joseph,\n",
      "Gronniosaw, James Albert\n",
      "Habberton, John\n",
      "Haldane, Elizabeth Sanderson,\n",
      "Hall, Edward B.\n",
      "Harland, Marion,\n",
      "Harrison, James\n",
      "Haynes, Henrietta.\n",
      "Higginson, Thomas Wentworth,\n",
      "Horne, C. Silvester\n",
      "Howe, Julia Ward,\n",
      "Hughes, Thomas\n",
      "Ingemann, Bernhard Severin\n",
      "Ingoldsby, Thomas\n",
      "Jenkins, Hester Donaldson,\n",
      "Johnson, Willis Fletcher,\n",
      "Keller, Elizabeth Leavitt\n",
      "Kemble, Fanny,\n",
      "Kennard, Nina H.,\n",
      "Kennedy, John Pendleton\n",
      "Kippis, Andrew\n",
      "Larsen, Hanna Astrup,\n",
      "Lever, Charles James\n",
      "Levinger, Lee J.\n",
      "Levy, T. Aaron,\n",
      "Longstreet, James,\n",
      "Loria, Achille,\n",
      "MacLane, Mary\n",
      "Marrs, William Taylor\n",
      "Marshall, John\n",
      "Mayer, Brantz,\n",
      "Miller, Florence Fenwick,\n",
      "Mixson, Frank M\n",
      "Moore, Thomas\n",
      "Morlae, Edward\n",
      "Morris, Clara,\n",
      "Moscheles, Felix,\n",
      "Moses, Belle.\n",
      "Norgate, Kate.\n",
      "O'Neil, Owen Rowe\n",
      "Oliphant, Mrs. (Margaret)\n",
      "Opie, Amelia Alderson\n",
      "Perryman, F. M\n",
      "Pompadour, Jeanne Antoinette\n",
      "Reed, John\n",
      "Reid, Mayne\n",
      "Reid, Stuart J.\n",
      "Reynolds, Robert Rice\n",
      "Roosevelt, Theodore,\n",
      "Ruxton, George Frederick Augustus\n",
      "Saxton, Evelyn\n",
      "Scoville, Samuel\n",
      "Sessions, Frederick.\n",
      "Shelley, Mary Wollstonecraft\n",
      "Sheridan, Clare\n",
      "Smeaton, William Henry Oliphant\n",
      "Smiles, Samuel,\n",
      "Smith, Lucy,\n",
      "Sterne, Elaine\n",
      "Stoddard, Charles Warren\n",
      "Stone, William L.\n",
      "Stowe, Harriet Beecher\n",
      "Stowe, Harriet Beecher,\n",
      "Stratton, Royal B\n",
      "Stretton, Hesba\n",
      "Tappan, Eva March\n",
      "Taylor, Meadows\n",
      "Tetley, William C\n",
      "Thoreau, Henry David\n",
      "Thwaites, Reuben Gold,\n",
      "Torrey, Bradford\n",
      "Trollope, Anthony\n",
      "Tyndall, John,\n",
      "Whiting, Lilian,\n",
      "Wordsworth, Dorothy,\n",
      "Yeats-Brown, Francis,\n",
      "de Stael, Germaine\n"
     ]
    }
   ],
   "source": [
    "authors = list(set(chunkmeta['author']))\n",
    "authors.sort()\n",
    "for a in authors:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-theta",
   "metadata": {},
   "source": [
    "Our next problem to resolve is, check out Stowe above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-version",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
