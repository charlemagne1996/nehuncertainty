{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "geographic-interim",
   "metadata": {},
   "source": [
    "# A template for experiments\n",
    "\n",
    "I'm trying to develop a pipeline we can use for experiments on the NEH data: a template we can use for selecting data, training a model, and finally evaluating results. But this is by no means set in stone yet; it's a draft we can discuss and adjust.\n",
    "\n",
    "For a first test, let's consider the problem of author gender. We know our data model of gender is imperfect (limited for the most part to m/f), and we don't imagine a predictive model trained on this boundary will tell us very much about gender directly; it's almost certainly, to some degree, a proxy for genre. But it's a tricky boundary to model and thus a good place to start. We're in no danger of getting 100% accuracy!\n",
    "\n",
    "Our ultimate goal in this experiment, and other experiments based on this template, is to figure out:\n",
    "\n",
    "1. What kinds of *historical questions* are really distorted by the errors in digital libraries? Our working hypothesis is that certain boundaries (like genre and date) are likely to be more sensitive than others (like e.g. gender), because the errors are in practice not distributed randomly.\n",
    "\n",
    "2. What kinds of *errors* are most likely to produce distortion? I'm acting on the hypothesis (or hunch) that paratext is at least as big a problem as the better-studied problem of OCR error. For instance, the paratext in works of fiction is often from a different genre, and composed at a different time, than the original work. So it's very likely to distort conclusions.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "solar-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import rankdata\n",
    "from scipy.stats import pearsonr\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-stupid",
   "metadata": {},
   "source": [
    "### Volume metadata\n",
    "\n",
    "This metadata file is in the GitHub repo. It has one row for each clean volume. \n",
    "\n",
    "Not all of these volumes have been trimmed and chunked; we will rely on the Box folders for a list of the chunks actually available to model. But then we will use this list in order to get features like genre, gender, and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "wrapped-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "volmeta = pd.read_csv('../metadata/updatedvolumemetadata.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "stuffed-layout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>author</th>\n",
       "      <th>authordate</th>\n",
       "      <th>title</th>\n",
       "      <th>latestcomp</th>\n",
       "      <th>hathidate</th>\n",
       "      <th>imprint</th>\n",
       "      <th>gutenstring</th>\n",
       "      <th>enumcron</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>...</th>\n",
       "      <th>contents</th>\n",
       "      <th>instances</th>\n",
       "      <th>genre</th>\n",
       "      <th>audience</th>\n",
       "      <th>authgender</th>\n",
       "      <th>multiplehtids</th>\n",
       "      <th>comments</th>\n",
       "      <th>coder</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Trimmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hvd.32044070870779</td>\n",
       "      <td>Smiles, Samuel,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lives of the engineers</td>\n",
       "      <td>1879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London;J. Murray;1874-1877.</td>\n",
       "      <td>Smiles, Samuel | Lives of the Engineers</td>\n",
       "      <td>v. 5</td>\n",
       "      <td>27710</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio | short</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 people, mixed together (not one per chapter)</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdp.39015005892362</td>\n",
       "      <td>Cruttwell, Maud.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Luca Signorelli</td>\n",
       "      <td>1899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London;G. Bell &amp; sons;1899.</td>\n",
       "      <td>Cruttwell, Maud | Luca Signorelli</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27759</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mdp.39015051108531</td>\n",
       "      <td>Bettany, George Thomas,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Life of Charles Darwin</td>\n",
       "      <td>1887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London;W. Scott;1887.</td>\n",
       "      <td>Bettany, George Thomas | Life of Charles Darwin</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>28380</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loc.ark+=13960=t6b27z54n</td>\n",
       "      <td>Gay, Sydney Howard,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>James Madison</td>\n",
       "      <td>1889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Boston;New York;Houghton, Mi</td>\n",
       "      <td>Gay, Sydney Howard | James Madison</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>28992</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docid                   author authordate  \\\n",
       "0  loc.ark+=13960=t5p851b8s          Reid, Stuart J.        NaN   \n",
       "1        hvd.32044070870779          Smiles, Samuel,        NaN   \n",
       "2        mdp.39015005892362         Cruttwell, Maud.        NaN   \n",
       "3        mdp.39015051108531  Bettany, George Thomas,        NaN   \n",
       "4  loc.ark+=13960=t6b27z54n      Gay, Sydney Howard,        NaN   \n",
       "\n",
       "                    title  latestcomp  hathidate  \\\n",
       "0       Lord John Russell        1895        NaN   \n",
       "1  Lives of the engineers        1879        NaN   \n",
       "2         Luca Signorelli        1899        NaN   \n",
       "3  Life of Charles Darwin        1887        NaN   \n",
       "4           James Madison        1889        NaN   \n",
       "\n",
       "                        imprint  \\\n",
       "0  New York;Harper & brothers;1   \n",
       "1   London;J. Murray;1874-1877.   \n",
       "2   London;G. Bell & sons;1899.   \n",
       "3         London;W. Scott;1887.   \n",
       "4  Boston;New York;Houghton, Mi   \n",
       "\n",
       "                                       gutenstring enumcron gbindex  ...  \\\n",
       "0              Reid, Stuart J. | Lord John Russell  <blank>   27553  ...   \n",
       "1          Smiles, Samuel | Lives of the Engineers     v. 5   27710  ...   \n",
       "2                Cruttwell, Maud | Luca Signorelli  <blank>   27759  ...   \n",
       "3  Bettany, George Thomas | Life of Charles Darwin  <blank>   28380  ...   \n",
       "4               Gay, Sydney Howard | James Madison  <blank>   28992  ...   \n",
       "\n",
       "   contents  instances        genre audience authgender  multiplehtids  \\\n",
       "0       NaN        NaN          bio      NaN          u            NaN   \n",
       "1       NaN        NaN  bio | short      NaN          m            NaN   \n",
       "2       NaN        NaN          bio      NaN          f            NaN   \n",
       "3       NaN        NaN          bio      NaN          m            NaN   \n",
       "4       NaN        NaN          bio      NaN          u            NaN   \n",
       "\n",
       "                                         comments   coder           Folder  \\\n",
       "0                                             NaN  morgan  gutenbiotrimmed   \n",
       "1  2 people, mixed together (not one per chapter)  morgan  gutenbiotrimmed   \n",
       "2                                             NaN  morgan  gutenbiotrimmed   \n",
       "3                                             NaN  morgan  gutenbiotrimmed   \n",
       "4                                             NaN  morgan  gutenbiotrimmed   \n",
       "\n",
       "   Trimmed  \n",
       "0  Trimmed  \n",
       "1  Trimmed  \n",
       "2  Trimmed  \n",
       "3  Trimmed  \n",
       "4  Trimmed  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volmeta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-lying",
   "metadata": {},
   "source": [
    "Here's a function we can use to generate a simpler genre column for modeling the biography / fiction boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "reported-power",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly dutch\n"
     ]
    }
   ],
   "source": [
    "def simplify_genre(genrestring):\n",
    "    genres = [x.strip() for x in genrestring.split('|')]\n",
    "    if 'bio' in genres:\n",
    "        return 'bio'\n",
    "    elif 'fic' in genres:\n",
    "        return 'fic'\n",
    "    else:\n",
    "        print('anomaly', genrestring)\n",
    "        return float('nan')\n",
    "\n",
    "volmeta['simplegenre'] = volmeta['genre'].apply(simplify_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-brake",
   "metadata": {},
   "source": [
    "### Works we have actually trimmed and chunked so far\n",
    "\n",
    "The Box folders ```cleannarratives``` and ```dirtynarratives``` store processed texts.\n",
    "\n",
    "The ```cleannarratives``` folder consists of Gutenberg texts, manually trimmed and then automatically chunked to contain ~80,000 characters each.\n",
    "\n",
    "The ```dirtynarratives``` folder contains Hathi texts that have *not* been trimmed. The only clean-up done on these so far is that running headers have been removed. They've also been chunked to contain ~80,000 characters each, but the boundaries are not aligned with ```cleannarratives.```\n",
    "\n",
    "Note that the number of files will not be the same in these two folders, and the number of chunks for each volume will not be the same. They are chunked independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "tender-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanfiles = [x for x in os.listdir('/Users/tunder/Box Sync/NEHproject/cleannarratives/')\n",
    "              if x.endswith('.txt')]\n",
    "dirtyfiles = [x for x in os.listdir('/Users/tunder/Box Sync/NEHproject/dirtynarratives/')\n",
    "               if x.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "clean-display",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2489 clean chunks, and\n",
      "2610  dirty ones.\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \" + str(len(cleanfiles)) + \" clean chunks, and\")\n",
    "print(str(len(dirtyfiles)), \" dirty ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-medication",
   "metadata": {},
   "source": [
    "What is actually in these data objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dramatic-morning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'36965_3.txt'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanfiles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-mills",
   "metadata": {},
   "source": [
    "How many *volumes* have we trimmed and chunked so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "sacred-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gbindex(filename):\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "gbdict = dict()\n",
    "\n",
    "for filename in cleanfiles:\n",
    "    gbindex = get_gbindex(filename)\n",
    "    if gbindex not in gbdict:\n",
    "        gbdict[gbindex] = []\n",
    "    gbdict[gbindex].append(filename)\n",
    "\n",
    "gbset = set(gbdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "reverse-buddy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 423 separate clean volumes that have been chunked.\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \" + str(len(gbset)) + \" separate clean volumes that have been chunked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-planning",
   "metadata": {},
   "source": [
    "Let's create a dataframe with just the volumes we're actually using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "hundred-hacker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(423, 25)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ourtitles = volmeta.loc[volmeta['gbindex'].isin(gbset), : ]\n",
    "ourtitles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-windsor",
   "metadata": {},
   "source": [
    "### Balancing the distribution of classes across time\n",
    "\n",
    "Language change is very easy to model, so if you try to model the boundary between two categories that happen to be distributed differently across time (in your collection), you're very likely to get a model of language change. That's a problem if you want to study the categorical difference in itself, separated from confounding issues of chronology that might just be selection bias.\n",
    "\n",
    "Here we're distinguishing books written by men from those written by women. And although we don't really care about the model in its own right (since we're interested in the consequences of OCR distortion), it's still important to know what we're modeling, because OCR distortion could have *different* effects on different kinds of boundaries (e.g. chronological or demographic). \n",
    "\n",
    "So we need to be careful to balance the classes across time. At a minimum, we should require the median date for both categories to be roughly the same. A more ambitious approach would match the full distribution. But for right now let's keep it simple.\n",
    "\n",
    "Also, in order to keep things simple, let's select both classes to be the same size. So we'll match the size of the smaller class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "shaped-pride",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smaller class has  139  volumes.\n"
     ]
    }
   ],
   "source": [
    "indexes_f = ourtitles.loc[ourtitles.authgender == 'f', : ].index.tolist()\n",
    "indexes_m = ourtitles.loc[ourtitles.authgender == 'm', : ].index.tolist()\n",
    "print('The smaller class has ', len(indexes_f), ' volumes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-gambling",
   "metadata": {},
   "source": [
    "The following function selects a matching number of volumes while keeping the median date similar in both categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "electric-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_medians(smaller_indexes, larger_indexes, metadata):\n",
    "    '''\n",
    "    smaller_indexes = indexes of metadata for the smaller class\n",
    "    larger_indexes = indexes of metadata for the larger class\n",
    "    '''\n",
    "    selected_from_large = []\n",
    "    smaller_median = np.median(metadata.loc[smaller_indexes, 'latestcomp'])\n",
    "    \n",
    "    largerdf = metadata.loc[larger_indexes, : ]\n",
    "    above_median = largerdf.loc[largerdf['latestcomp'] >= smaller_median, : ].index.tolist()\n",
    "    below_median = largerdf.loc[largerdf['latestcomp'] <= smaller_median, : ].index.tolist()\n",
    "    \n",
    "    for i in range(len(smaller_indexes)):\n",
    "        if len(selected_from_large) > 0:\n",
    "            larger_median = np.median(metadata.loc[selected_from_large, 'latestcomp'])\n",
    "        else:\n",
    "            larger_median = smaller_median\n",
    "        \n",
    "        if larger_median >= smaller_median and len(below_median) > 0:\n",
    "            selected = random.sample(below_median, 1)[0]\n",
    "            below_median.pop(below_median.index(selected))\n",
    "            selected_from_large.append(selected)\n",
    "        elif larger_median <= smaller_median and len(above_median) > 0:\n",
    "            selected = random.sample(above_median, 1)[0]\n",
    "            above_median.pop(above_median.index(selected))\n",
    "            selected_from_large.append(selected)\n",
    "        else:\n",
    "            # we have no more items that won't distort the median\n",
    "            break\n",
    "\n",
    "    return selected_from_large, smaller_median, larger_median\n",
    "\n",
    "selected_m, median_f, median_m = match_medians(indexes_f, indexes_m, ourtitles)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "olive-threshold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1890.0 1889.5 139\n"
     ]
    }
   ],
   "source": [
    "print(median_f, median_m, len(selected_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dominant-uniform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_selected_vols = indexes_f + selected_m\n",
    "len(all_selected_vols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-heater",
   "metadata": {},
   "source": [
    "We've gathered the actual indexes of rows in ```ourtitles.``` Now let's convert those to Gutenberg indexes (or other clean-volume identifiers, in cases where volumes are drawn from ECCO, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "progressive-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_gbindexes = ourtitles.loc[all_selected_vols, 'gbindex']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-moldova",
   "metadata": {},
   "source": [
    "### Merging chunk and volume\n",
    "\n",
    "So far we've been operating at the volume level. But we will need chunk-level metadata in order to actually model the files.\n",
    "\n",
    "Let's create dataframes that have a row for each chunk and merge those dataframes with the information in volmeta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "comparable-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunkframe(filelist, volmeta):\n",
    "    \n",
    "    chunkids = []\n",
    "    gbindices = []\n",
    "    \n",
    "    for filename in filelist:\n",
    "        chunkids.append(filename.replace('.txt', ''))\n",
    "        gbindices.append(get_gbindex(filename))\n",
    "    \n",
    "    df = pd.DataFrame({'chunkid': chunkids, 'gbindex': gbindices})\n",
    "    \n",
    "    chunkmeta = df.merge(volmeta, how = 'inner', on = 'gbindex')\n",
    "    \n",
    "    return chunkmeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "arabic-dubai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2489, 26)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanmeta = create_chunkframe(cleanfiles, ourtitles)\n",
    "cleanmeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "outstanding-jacob",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2610, 26)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtymeta = create_chunkframe(dirtyfiles, ourtitles)\n",
    "dirtymeta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-copying",
   "metadata": {},
   "source": [
    "Now we select the subsets of those dataframes with Gutenberg indexes that were selected by the match_medians function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "present-spirituality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 26)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanmodelmeta = cleanmeta.loc[cleanmeta['gbindex'].isin(selected_gbindexes), : ]\n",
    "cleanmodelmeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "educational-excuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1624, 26)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtymodelmeta = dirtymeta.loc[dirtymeta['gbindex'].isin(selected_gbindexes), : ]\n",
    "dirtymodelmeta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-church",
   "metadata": {},
   "source": [
    "### Term-document matrices for clean and dirty narratives \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "protecting-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rootdir = '/Users/tunder/Box Sync/NEHproject/cleannarratives/'\n",
    "\n",
    "clean_paths = []\n",
    "\n",
    "for chunk_id in cleanmodelmeta['chunkid']:\n",
    "    clean_paths.append(clean_rootdir + chunk_id + '.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "persistent-indiana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>11th</th>\n",
       "      <th>12</th>\n",
       "      <th>12th</th>\n",
       "      <th>13</th>\n",
       "      <th>13th</th>\n",
       "      <th>...</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youths</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zealously</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zest</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zone</th>\n",
       "      <th>æsthetic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chunkid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36965_3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 17110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         000  10  100  10th  11  11th  12  12th  13  13th  ...  youthful  \\\n",
       "chunkid                                                    ...             \n",
       "36965_3    0   1    0     0   0     0   0     0   0     0  ...         0   \n",
       "36965_2    0   2    0     0   2     0   2     1   2     0  ...         0   \n",
       "36965_0    0   0    0     0   0     0   0     1   0     0  ...         2   \n",
       "36965_1    0   0    0     0   0     0   0     0   0     0  ...         1   \n",
       "36965_4    0   2    0     1   1     0   0     0   0     0  ...         0   \n",
       "\n",
       "         youths  zeal  zealous  zealously  zenith  zest  zigzag  zone  \\\n",
       "chunkid                                                                 \n",
       "36965_3       0     0        0          0       0     0       0     0   \n",
       "36965_2       0     1        0          0       0     0       0     0   \n",
       "36965_0       0     0        0          0       0     0       0     0   \n",
       "36965_1       0     0        0          0       0     0       0     0   \n",
       "36965_4       0     0        0          0       0     0       0     0   \n",
       "\n",
       "         æsthetic  \n",
       "chunkid            \n",
       "36965_3         0  \n",
       "36965_2         0  \n",
       "36965_0         0  \n",
       "36965_1         0  \n",
       "36965_4         0  \n",
       "\n",
       "[5 rows x 17110 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_model_chunk_ids = cleanmodelmeta['chunkid']\n",
    "\n",
    "vectorizer = CountVectorizer(input = 'filename', min_df = .02)\n",
    "sparse_clean_counts = vectorizer.fit_transform(clean_paths) # the vectorizer produces something\n",
    "                                                               # called a 'sparse matrix'; we need to\n",
    "                                                               # unpack it\n",
    "clean_wordcounts = pd.DataFrame(sparse_clean_counts.toarray(), index = clean_model_chunk_ids, \n",
    "                            columns = vectorizer.get_feature_names())\n",
    "clean_wordcounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "continuing-helena",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>...</th>\n",
       "      <th>ſuſpect</th>\n",
       "      <th>ſuſpected</th>\n",
       "      <th>ſuſpicion</th>\n",
       "      <th>ſwear</th>\n",
       "      <th>ſweet</th>\n",
       "      <th>ſwered</th>\n",
       "      <th>ſwore</th>\n",
       "      <th>ſº</th>\n",
       "      <th>ﬁnd</th>\n",
       "      <th>ﬁrst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chunkid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36965_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19949 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00  000  01  10  100  101  102  103  104  105  ...  ſuſpect  \\\n",
       "chunkid                                                 ...            \n",
       "36965_3   0    0   0   1    0    0    0    0    0    0  ...        0   \n",
       "36965_2   0    0   0   0    0    0    0    0    0    0  ...        0   \n",
       "36965_0   0    0   0   0    1    0    0    0    0    0  ...        0   \n",
       "36965_1   0    0   0   0    0    0    0    0    0    0  ...        0   \n",
       "36965_4   3    0   0   1    0    0    0    0    0    0  ...        0   \n",
       "\n",
       "         ſuſpected  ſuſpicion  ſwear  ſweet  ſwered  ſwore  ſº  ﬁnd  ﬁrst  \n",
       "chunkid                                                                    \n",
       "36965_3          0          0      0      0       0      0   0    0     0  \n",
       "36965_2          0          0      0      0       0      0   0    0     0  \n",
       "36965_0          0          0      0      0       0      0   0    0     0  \n",
       "36965_1          0          0      0      0       0      0   0    0     0  \n",
       "36965_4          0          0      0      0       0      0   0    0     0  \n",
       "\n",
       "[5 rows x 19949 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty_rootdir = '/Users/tunder/Box Sync/NEHproject/dirtynarratives/'\n",
    "\n",
    "dirty_paths = []\n",
    "\n",
    "for chunk_id in dirtymodelmeta['chunkid']:\n",
    "    dirty_paths.append(dirty_rootdir + chunk_id + '.txt')\n",
    "\n",
    "dirty_model_chunk_ids = dirtymodelmeta['chunkid']\n",
    "    \n",
    "vectorizer = CountVectorizer(input = 'filename', min_df = .02)\n",
    "sparse_dirty_counts = vectorizer.fit_transform(dirty_paths) # the vectorizer produces something\n",
    "                                                               # called a 'sparse matrix'; we need to\n",
    "                                                               # unpack it\n",
    "dirty_wordcounts = pd.DataFrame(sparse_dirty_counts.toarray(), index = dirty_model_chunk_ids, \n",
    "                            columns = vectorizer.get_feature_names())\n",
    "dirty_wordcounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-exchange",
   "metadata": {},
   "source": [
    "### Turn wordcounts into normalized frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "prescribed-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rowsums = clean_wordcounts.sum(axis = 'columns')\n",
    "clean_freqs = clean_wordcounts.divide(clean_rowsums, axis = 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "collaborative-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_rowsums = dirty_wordcounts.sum(axis = 'columns')\n",
    "dirty_freqs = dirty_wordcounts.divide(dirty_rowsums, axis = 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "honest-award",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>...</th>\n",
       "      <th>ſuſpect</th>\n",
       "      <th>ſuſpected</th>\n",
       "      <th>ſuſpicion</th>\n",
       "      <th>ſwear</th>\n",
       "      <th>ſweet</th>\n",
       "      <th>ſwered</th>\n",
       "      <th>ſwore</th>\n",
       "      <th>ſº</th>\n",
       "      <th>ﬁnd</th>\n",
       "      <th>ﬁrst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chunkid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36965_3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_4</th>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19949 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               00  000   01        10       100  101  102  103  104  105  ...  \\\n",
       "chunkid                                                                   ...   \n",
       "36965_3  0.000000  0.0  0.0  0.000074  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "36965_2  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "36965_0  0.000000  0.0  0.0  0.000000  0.000073  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "36965_1  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "36965_4  0.000216  0.0  0.0  0.000072  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "         ſuſpect  ſuſpected  ſuſpicion  ſwear  ſweet  ſwered  ſwore   ſº  ﬁnd  \\\n",
       "chunkid                                                                         \n",
       "36965_3      0.0        0.0        0.0    0.0    0.0     0.0    0.0  0.0  0.0   \n",
       "36965_2      0.0        0.0        0.0    0.0    0.0     0.0    0.0  0.0  0.0   \n",
       "36965_0      0.0        0.0        0.0    0.0    0.0     0.0    0.0  0.0  0.0   \n",
       "36965_1      0.0        0.0        0.0    0.0    0.0     0.0    0.0  0.0  0.0   \n",
       "36965_4      0.0        0.0        0.0    0.0    0.0     0.0    0.0  0.0  0.0   \n",
       "\n",
       "         ﬁrst  \n",
       "chunkid        \n",
       "36965_3   0.0  \n",
       "36965_2   0.0  \n",
       "36965_0   0.0  \n",
       "36965_1   0.0  \n",
       "36965_4   0.0  \n",
       "\n",
       "[5 rows x 19949 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty_freqs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-criterion",
   "metadata": {},
   "source": [
    "Notice the eighteenth-century long S's!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-profit",
   "metadata": {},
   "source": [
    "### Separate train & validation from final test set\n",
    "\n",
    "We will optimize our model by cross-validating on 3/4 of the data, and finally test on a held-out 1/4. The same volumes should be held out for both clean and dirty data. Note that we select *by author* to avoid leakage across this boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "virgin-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_authors = list(set(volmeta.loc[volmeta['gbindex'].isin(selected_gbindexes), 'author']))\n",
    "random.shuffle(selected_authors)\n",
    "threequarters = int(len(selected_authors) * .75)\n",
    "trainauthors = selected_authors[0 : threequarters]\n",
    "testauthors = selected_authors[threequarters: ]\n",
    "\n",
    "cleantrain = cleanmodelmeta.loc[cleanmodelmeta['author'].isin(trainauthors), : ]\n",
    "cleantest = cleanmodelmeta.loc[cleanmodelmeta['author'].isin(testauthors), : ]\n",
    "cleantrain_freqs = clean_freqs.loc[cleantrain.chunkid]\n",
    "cleantest_freqs = clean_freqs.loc[cleantest.chunkid]\n",
    "\n",
    "dirtytrain = dirtymodelmeta.loc[dirtymodelmeta['author'].isin(trainauthors), : ]\n",
    "dirtytest = dirtymodelmeta.loc[dirtymodelmeta['author'].isin(testauthors), : ]\n",
    "dirtytrain_freqs = dirty_freqs.loc[dirtytrain.chunkid]\n",
    "dirtytest_freqs = dirty_freqs.loc[dirtytest.chunkid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "split-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean training set:  (1177, 26)\n",
      "Clean test set:  (386, 26)\n",
      "Hathi training set:  (1231, 26)\n",
      "Hathi testing set:  (393, 26)\n"
     ]
    }
   ],
   "source": [
    "print('Clean training set: ', cleantrain.shape)\n",
    "print('Clean test set: ', cleantest.shape)\n",
    "print('Hathi training set: ', dirtytrain.shape)\n",
    "print('Hathi testing set: ', dirtytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-conditions",
   "metadata": {},
   "source": [
    "Let's put our master modelmeta frames in the right order to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "theoretical-mining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1624, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1624, 26)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtymodelmeta.set_index('chunkid', inplace = True)\n",
    "print(dirtymodelmeta.shape)\n",
    "dirty_ids_order = np.append(list(dirtytrain.chunkid), list(dirtytest.chunkid))\n",
    "dirtymodelmeta = dirtymodelmeta.loc[dirty_ids_order, : ]\n",
    "dirtymodelmeta.reset_index(inplace = True)\n",
    "dirtymodelmeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "retired-argentina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1563, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1563, 26)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanmodelmeta.set_index('chunkid', inplace = True)\n",
    "print(cleanmodelmeta.shape)\n",
    "clean_ids_order = np.append(list(cleantrain.chunkid), list(cleantest.chunkid))\n",
    "cleanmodelmeta = cleanmodelmeta.loc[clean_ids_order, : ]\n",
    "cleanmodelmeta.reset_index(inplace = True)\n",
    "cleanmodelmeta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-minimum",
   "metadata": {},
   "source": [
    "### Let's produce a model for the clean counts\n",
    "\n",
    "We're going to do a grid search for the best model. The outer loop will select the number of features. The inner loop will select the regularization constant.\n",
    "\n",
    "In selecting the top *n* features, we will always select the *n* with top *document* frequency.\n",
    "\n",
    "We will do the cross-validation only on the training set. This means in effect that there are lots of different \"validation sets\" inside the training set. But the test set itself is a separate issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "proprietary-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_freqs(X, y):\n",
    "    return np.count_nonzero(X, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "advised-surgery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 1000000 0.717094017094017\n",
      "2000 100000 0.7179414747211357\n",
      "2000 10000 0.7213313052296103\n",
      "2000 3000 0.7289729103288425\n",
      "2000 1000 0.7298348544111256\n",
      "2000 500 0.725583079820368\n",
      "2000 100 0.724728378965667\n",
      "2000 10 0.7238736781109663\n",
      "2000 1 0.730675068810662\n",
      "2000 0.1 0.733253657829929\n",
      "2500 1000000 0.7281326959293061\n",
      "2500 100000 0.7298348544111256\n",
      "2500 10000 0.7357887874837028\n",
      "2500 3000 0.7468274663189918\n",
      "2500 1000 0.7468274663189918\n",
      "2500 500 0.7459944951470375\n",
      "2500 100 0.7451397942923367\n",
      "2500 10 0.7468347095465739\n",
      "2500 1 0.7451180646095901\n",
      "2500 0.1 0.7535926408807765\n",
      "3000 1000000 0.7272707518470231\n",
      "3000 100000 0.7281254527017239\n",
      "3000 10000 0.7298203679559612\n",
      "3000 3000 0.7502245400550486\n",
      "3000 1000 0.7493698392003477\n",
      "3000 500 0.7468129798638273\n",
      "3000 100 0.7468202230914095\n",
      "3000 10 0.7468347095465739\n",
      "3000 1 0.7510792409097494\n",
      "3000 0.1 0.7536143705635231\n",
      "3500 1000000 0.7298276111835434\n",
      "3500 100000 0.730675068810662\n",
      "3500 10000 0.7366290018832391\n",
      "3500 3000 0.7459727654642909\n",
      "3500 1000 0.7502245400550486\n",
      "3500 500 0.7459872519194553\n",
      "3500 100 0.7425901781833986\n",
      "3500 10 0.7417427205562799\n",
      "3500 1 0.7417354773286977\n",
      "3500 0.1 0.7468129798638273\n",
      "3800 1000000 0.747660437490946\n",
      "3800 100000 0.7493625959727656\n",
      "3800 10000 0.7502172968274664\n",
      "3800 3000 0.7535998841083587\n",
      "3800 1000 0.7544473417354773\n",
      "3800 500 0.7510502679994205\n",
      "3800 100 0.7468057366362452\n",
      "3800 10 0.7459655222367086\n",
      "3800 1 0.7459582790091265\n",
      "3800 0.1 0.7468057366362452\n",
      "4000 1000000 0.7612414892075909\n",
      "4000 100000 0.7603867883528901\n",
      "4000 10000 0.7612487324351731\n",
      "4000 3000 0.7654860205707663\n",
      "4000 1000 0.7620961900622918\n",
      "4000 500 0.7604012748080544\n",
      "4000 100 0.7612487324351731\n",
      "4000 10 0.7604012748080544\n",
      "4000 1 0.7612487324351731\n",
      "4000 0.1 0.7620889468347095\n",
      "4200 1000000 0.752687237433\n",
      "4200 100000 0.7509923221787629\n",
      "4200 10000 0.753541938287701\n",
      "4200 3000 0.7526872374330001\n",
      "4200 1000 0.7560915543966391\n",
      "4200 500 0.7552513399971027\n",
      "4200 100 0.7518542662610459\n",
      "4200 10 0.750999565406345\n",
      "4200 1 0.750999565406345\n",
      "4200 0.1 0.7543893959148196\n",
      "4500 1000000 0.7399681297986384\n",
      "4500 100000 0.7408155874257569\n",
      "4500 10000 0.7408083441981747\n",
      "4500 3000 0.7442199043893959\n",
      "4500 1000 0.7433579603071129\n",
      "4500 500 0.7425105026799942\n",
      "4500 100 0.742503259452412\n",
      "4500 10 0.7416702882804578\n",
      "4500 1 0.7399608865710562\n",
      "4500 0.1 0.7348761408083442\n",
      "5000 1000000 0.7297696653628858\n",
      "5000 100000 0.7297696653628858\n",
      "5000 10000 0.7297696653628857\n",
      "5000 3000 0.7314718238447052\n",
      "5000 1000 0.7348761408083442\n",
      "5000 500 0.729776908590468\n",
      "5000 100 0.7255468636824569\n",
      "5000 10 0.7272562653918586\n",
      "5000 1 0.7272779950746052\n",
      "5000 0.1 0.7315225264377807\n",
      "5500 1000000 0.7306460959003332\n",
      "5500 100000 0.7306460959003332\n",
      "5500 10000 0.7314863102998697\n",
      "5500 3000 0.7323482543821528\n",
      "5500 1000 0.7357598145733739\n",
      "5500 500 0.7357525713457918\n",
      "5500 100 0.7357670578009562\n",
      "5500 10 0.7357743010285384\n",
      "5500 1 0.7340721425467188\n",
      "5500 0.1 0.7340721425467189\n",
      "6000 1000000 0.7323772272924816\n",
      "6000 100000 0.7323772272924816\n",
      "6000 10000 0.7340793857743011\n",
      "6000 3000 0.7332174416920181\n",
      "6000 1000 0.7272707518470231\n",
      "6000 500 0.7298276111835433\n",
      "6000 100 0.7306823120382442\n",
      "6000 10 0.7306823120382443\n",
      "6000 1 0.7315297696653629\n",
      "6000 0.1 0.7289946400115892\n",
      "6500 1000000 0.7332029552368535\n",
      "6500 100000 0.7332029552368535\n",
      "6500 10000 0.732355497609735\n",
      "6500 3000 0.7332101984644358\n",
      "6500 1000 0.7400115891641315\n",
      "6500 500 0.7366072722004926\n",
      "6500 100 0.7366217586556569\n",
      "6500 10 0.7366217586556569\n",
      "6500 1 0.7349196001738375\n",
      "6500 0.1 0.7349196001738376\n",
      "7000 1000000 0.7298058815007968\n",
      "7000 100000 0.7306533391279154\n",
      "7000 10000 0.7315007967550341\n",
      "7000 3000 0.7357525713457918\n",
      "7000 1000 0.7332246849196002\n",
      "7000 500 0.7315442561205272\n",
      "7000 100 0.7298420976387078\n",
      "7000 10 0.7290018832391714\n",
      "7000 1 0.7298420976387078\n",
      "7000 0.1 0.7281399391568883\n",
      "7500 1000000 0.7289801535564248\n",
      "7500 100000 0.7289801535564248\n",
      "7500 10000 0.730675068810662\n",
      "7500 3000 0.7306823120382443\n",
      "7500 1000 0.7289873967840069\n",
      "7500 500 0.7272779950746052\n",
      "7500 100 0.7281399391568882\n",
      "7500 10 0.725583079820368\n",
      "7500 1 0.7247283789656671\n",
      "7500 0.1 0.7264232942199044\n",
      "8000 1000000 0.7213095755468638\n",
      "8000 100000 0.7213095755468638\n",
      "8000 10000 0.7221642764015644\n",
      "8000 3000 0.7247066492829205\n",
      "8000 1000 0.7306678255830799\n",
      "8000 500 0.7298203679559612\n",
      "8000 100 0.7315152832101985\n",
      "8000 10 0.7298203679559612\n",
      "8000 1 0.729813124728379\n",
      "8000 0.1 0.729813124728379\n"
     ]
    }
   ],
   "source": [
    "resultarray = []\n",
    "\n",
    "featureoptions = [2000, 2500, 3000, 3500, 3800, 4000, 4200, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000]\n",
    "c_options = [1000000, 100000, 10000, 3000, 1000, 500, 100, 10, 1, .1]\n",
    "\n",
    "for featurecount in featureoptions:\n",
    "    docfreqs = []\n",
    "    for col in cleantrain_freqs.columns:\n",
    "        docfreqs.append((sum(cleantrain_freqs[col] > 0), col))\n",
    "    docfreqs.sort()\n",
    "    features = [x[1] for x in docfreqs[-featurecount : ]] # because sorted in ascending order\n",
    "    \n",
    "    model_features = cleantrain_freqs.loc[ : , features]\n",
    "    \n",
    "    resultrow = []\n",
    "    \n",
    "    for c_param in c_options:\n",
    "        logreg = LogisticRegression(C = c_param, max_iter = 2000)\n",
    "        scaler = StandardScaler()\n",
    "        # feature_selector = SelectKBest(get_doc_freqs, k = featurecount)\n",
    "        pipe = Pipeline([\n",
    "            # ('fkb', feature_selector),\n",
    "            ('sc', scaler),\n",
    "            ('lr', logreg)\n",
    "        ])    \n",
    "        grouper = GroupKFold(n_splits = 10)\n",
    "        cv_results = cross_validate(estimator = pipe, \n",
    "                                    X = model_features,\n",
    "                                    y = cleantrain['authgender'], \n",
    "                                    groups = cleantrain['author'], \n",
    "                                    cv = grouper)\n",
    "        mean_score = np.mean(cv_results['test_score'])\n",
    "        print(featurecount, c_param, mean_score)\n",
    "        resultrow.append(mean_score)\n",
    "    \n",
    "    resultarray.append(resultrow)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-remains",
   "metadata": {},
   "source": [
    "#### visualize the grid search\n",
    "\n",
    "This can be a good way to check that you've really covered the space and \"surrounded\" the optimum value on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "coupled-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultarray = np.array(resultarray)\n",
    "thearrayshape = resultarray.shape\n",
    "resarray = rankdata(resultarray).reshape(thearrayshape)   # I'm doing this because otherwise\n",
    "                                                    # fine details at top of range can be hard to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "opened-termination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.imshow(resarray, cmap='hot', aspect = 'auto')\n",
    "plt.yticks(ticks = [x for x in range(len(featureoptions))], labels = featureoptions)\n",
    "thexlabels = [str(x) for x in c_options]\n",
    "thexlabels[1] = '10^6'\n",
    "thexlabels[1] = '10^5'\n",
    "thexlabels[2] = '10^4'\n",
    "plt.xticks(ticks = [x for x in range(len(c_options))], labels = thexlabels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "about-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "docfreqs = []\n",
    "for col in clean_wordcounts.columns:\n",
    "    docfreqs.append((sum(clean_wordcounts[col] > 0), col))\n",
    "docfreqs.sort()\n",
    "features = [x[1] for x in docfreqs[-4000: ]] #because sorted ascending\n",
    "\n",
    "train_features = cleantrain_freqs.loc[ : , features]  \n",
    "    \n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "bestmodel = LogisticRegression(C = 3000, max_iter = 2000)\n",
    "cleantrain_probabilities = cross_val_predict(bestmodel, train_features, cleantrain['authgender'], \n",
    "                                     groups = cleantrain['author'], cv = grouper,\n",
    "                                    method = 'predict_proba')\n",
    "\n",
    "## NOW APPLY THE SAME SCALER AND MODEL TO TEST SET\n",
    "\n",
    "bestmodel = LogisticRegression(C = 3000, max_iter = 2000)\n",
    "bestmodel.fit(train_features, cleantrain['authgender'])\n",
    "\n",
    "test_features = cleantest_freqs.loc[ : , features] \n",
    "test_features = scaler.transform(test_features) # Note this is the same scaler we fit\n",
    "                                                # to train features; we DON'T fit a new one to the\n",
    "                                                # test features. We deliberately blind ourselves to that\n",
    "                                                # information.\n",
    "\n",
    "cleantest_predictions = bestmodel.predict(test_features)\n",
    "cleantest_probabilities = bestmodel.predict_proba(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "hired-pocket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8031088082901554"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cleantest_predictions == cleantest['authgender']) / len(cleantest['authgender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-consumer",
   "metadata": {},
   "source": [
    "### Now a model for the dirty counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "attached-accessory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 1000000 0.7334740297429414\n",
      "3500 100000 0.7342870378730227\n",
      "3500 10000 0.7383455220062511\n",
      "3500 3000 0.7448629150490342\n",
      "3500 1000 0.7464758182748407\n",
      "3500 500 0.7424038986555915\n",
      "3500 100 0.7448429230458353\n",
      "3500 10 0.7432234633028509\n",
      "3500 1 0.7464623827888199\n",
      "3500 0.1 0.7481082835682932\n",
      "4000 1000000 0.7309620237926334\n",
      "4000 100000 0.7301490156625521\n",
      "4000 10000 0.7309554672754554\n",
      "4000 3000 0.7342010507624906\n",
      "4000 1000 0.7390658790247342\n",
      "4000 500 0.7390725430258003\n",
      "4000 100 0.7382660914128972\n",
      "4000 10 0.7390725430258005\n",
      "4000 1 0.7423246830300139\n",
      "4000 0.1 0.7382727554139634\n",
      "4500 1000000 0.7415443500019346\n",
      "4500 100000 0.742357358132016\n",
      "4500 10000 0.7447832694879037\n",
      "4500 3000 0.7455963851018732\n",
      "4500 1000 0.7456162696211838\n",
      "4500 500 0.74562938265554\n",
      "4500 100 0.7431770302631635\n",
      "4500 10 0.744796490006148\n",
      "4500 1 0.7447899334889698\n",
      "4500 0.1 0.7472488423985245\n",
      "5000 1000000 0.746362852708379\n",
      "5000 100000 0.7455365165761652\n",
      "5000 10000 0.7447300649632622\n",
      "5000 3000 0.7496017721943481\n",
      "5000 1000 0.7479954254857197\n",
      "5000 500 0.7447565059997506\n",
      "5000 100 0.7423240381266849\n",
      "5000 10 0.7423173741256186\n",
      "5000 1 0.7439433903857812\n",
      "5000 0.1 0.7463825222599133\n",
      "5500 1000000 0.7431043711547639\n",
      "5500 100000 0.7422913630246827\n",
      "5500 10000 0.7430978146375857\n",
      "5500 3000 0.7439173792848452\n",
      "5500 1000 0.7488086485835773\n",
      "5500 500 0.7471891888405928\n",
      "5500 100 0.7423308096116392\n",
      "5500 10 0.739885121220329\n",
      "5500 1 0.7406981293504104\n",
      "5500 0.1 0.742324145610573\n",
      "6000 1000000 0.754526146530635\n",
      "6000 100000 0.7537131384005538\n",
      "6000 10000 0.7561456062736196\n",
      "6000 3000 0.7537065818833757\n",
      "6000 1000 0.7529065793037624\n",
      "6000 500 0.7521001276908592\n",
      "6000 100 0.7472416409780174\n",
      "6000 10 0.7439896084576922\n",
      "6000 1 0.7440027214920482\n",
      "6000 0.1 0.7423701487147076\n",
      "6500 1000000 0.7415044734794255\n",
      "6500 100000 0.7415044734794255\n",
      "6500 10000 0.7423242530944612\n",
      "6500 3000 0.7480021969706742\n",
      "6500 1000 0.7496283207147248\n",
      "6500 500 0.7504413288448062\n",
      "6500 100 0.7520673451049688\n",
      "6500 10 0.7488217616179335\n",
      "6500 1 0.7463827372276894\n",
      "6500 0.1 0.7463958502620457\n",
      "7000 1000000 0.7415310219998021\n",
      "7000 100000 0.7423374736127055\n",
      "7000 10000 0.743950376838512\n",
      "7000 3000 0.747195852841659\n",
      "7000 1000 0.7488415386533559\n",
      "7000 500 0.7480219740060965\n",
      "7000 100 0.7488283181351115\n",
      "7000 10 0.7496413262651929\n",
      "7000 1 0.7512740065264216\n",
      "7000 0.1 0.7472352994286157\n",
      "7500 1000000 0.7463760732266232\n",
      "7500 100000 0.7463760732266232\n",
      "7500 10000 0.7447567209675269\n",
      "7500 3000 0.7471826323234148\n",
      "7500 1000 0.7488348746522896\n",
      "7500 500 0.752100127690859\n",
      "7500 100 0.749654439299549\n",
      "7500 10 0.7496609958167271\n",
      "7500 1 0.7496609958167271\n",
      "7500 0.1 0.7472219714264832\n",
      "8000 1000000 0.7358129866333036\n",
      "8000 100000 0.7358129866333036\n",
      "8000 10000 0.7398649142493541\n",
      "8000 3000 0.7390389005688046\n",
      "8000 1000 0.7398650217332421\n",
      "8000 500 0.736632551280563\n",
      "8000 100 0.7350196480547566\n",
      "8000 10 0.7374455594106444\n",
      "8000 1 0.7390715756708068\n",
      "8000 0.1 0.7398912478019545\n",
      "8500 1000000 0.7276695773303581\n",
      "8500 100000 0.7276695773303581\n",
      "8500 10000 0.730108601720602\n",
      "8500 3000 0.7325410695936678\n",
      "8500 1000 0.732547733594734\n",
      "8500 500 0.7341737498548967\n",
      "8500 100 0.7357997661150593\n",
      "8500 10 0.7366127742451406\n",
      "8500 1 0.7358064301161257\n",
      "8500 0.1 0.7349868654688663\n",
      "9000 1000000 0.7342067474085635\n",
      "9000 100000 0.7342067474085635\n",
      "9000 10000 0.7325740671473346\n",
      "9000 3000 0.7325543975958004\n",
      "9000 1000 0.7317545025000752\n",
      "9000 500 0.7342000834074971\n",
      "9000 100 0.7325806236645127\n",
      "9000 10 0.7325740671473346\n",
      "9000 1 0.7309546074043501\n",
      "9000 0.1 0.7285155830141061\n"
     ]
    }
   ],
   "source": [
    "resultarray = []\n",
    "\n",
    "featureoptions = [3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000]\n",
    "c_options = [1000000, 100000, 10000, 3000, 1000, 500, 100, 10, 1, .1]\n",
    "\n",
    "for featurecount in featureoptions:\n",
    "    docfreqs = []\n",
    "    for col in dirtytrain_freqs.columns:\n",
    "        docfreqs.append((sum(dirtytrain_freqs[col] > 0), col))\n",
    "    docfreqs.sort()\n",
    "    features = [x[1] for x in docfreqs[-featurecount: ]]  # because sorted ascending\n",
    "    \n",
    "    model_features = dirtytrain_freqs.loc[ : , features]\n",
    "    \n",
    "    resultrow = []\n",
    "    \n",
    "    for c_param in c_options:\n",
    "        logreg = LogisticRegression(C = c_param, max_iter = 2000)\n",
    "        scaler = StandardScaler()\n",
    "        # feature_selector = SelectKBest(get_doc_freqs, k = featurecount)\n",
    "        pipe = Pipeline([\n",
    "            # ('fs', feature_selector),\n",
    "            ('sc', scaler),\n",
    "            ('lr', logreg)\n",
    "        ])\n",
    "        grouper = GroupKFold(n_splits = 10)\n",
    "        cv_results = cross_validate(estimator = pipe, \n",
    "                                    X = model_features, \n",
    "                                    y = dirtytrain['authgender'], \n",
    "                                    groups = dirtytrain['author'], \n",
    "                                    cv = grouper)\n",
    "        mean_score = np.mean(cv_results['test_score'])\n",
    "        print(featurecount, c_param, mean_score)\n",
    "        resultrow.append(mean_score)\n",
    "    \n",
    "    resultarray.append(resultrow)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "norwegian-domain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultarray = np.array(resultarray)\n",
    "thearrayshape = resultarray.shape\n",
    "resarray = rankdata(resultarray).reshape(thearrayshape)   # I'm doing this because otherwise\n",
    "                                                    # fine details at top of range can be hard to see.\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax.imshow(resarray, cmap='hot', aspect = 'auto')\n",
    "plt.yticks(ticks = [x for x in range(len(featureoptions))], labels = featureoptions)\n",
    "thexlabels = [str(x) for x in c_options]\n",
    "thexlabels[1] = '10^6'\n",
    "thexlabels[1] = '10^5'\n",
    "thexlabels[2] = '10^4'\n",
    "plt.xticks(ticks = [x for x in range(len(c_options))], labels = thexlabels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "mechanical-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "docfreqs = []\n",
    "for col in dirtytrain_freqs.columns:\n",
    "    docfreqs.append((sum(dirtytrain_freqs[col] > 0), col))\n",
    "docfreqs.sort()\n",
    "features = [x[1] for x in docfreqs[-6000 : ]]\n",
    "\n",
    "train_features = dirtytrain_freqs.loc[ : , features]\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "bestmodel = LogisticRegression(C = 10000, max_iter = 2000)\n",
    "dirtytrain_probabilities = cross_val_predict(bestmodel, train_features, dirtytrain['authgender'], \n",
    "                                     groups = dirtytrain['author'], cv = grouper,\n",
    "                                    method = 'predict_proba')\n",
    "\n",
    "## NOW APPLY THE SAME SCALER AND MODEL TO TEST SET\n",
    "\n",
    "bestmodel = LogisticRegression(C = 10000, max_iter = 2000)\n",
    "bestmodel.fit(train_features, dirtytrain['authgender'])\n",
    "\n",
    "test_features = dirtytest_freqs.loc[ : , features] \n",
    "test_features = scaler.transform(test_features)    # Note this is the same scaler we fit\n",
    "                                                # to train features; we DON'T fit a new one to the\n",
    "                                                # test features. We deliberately blind ourselves to that\n",
    "                                                # information.\n",
    "\n",
    "dirtytest_predictions = bestmodel.predict(test_features)\n",
    "dirtytest_probabilities = bestmodel.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "collectible-hydrogen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7786259541984732"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dirtytest_predictions == dirtytest['authgender']) / len(dirtytest['authgender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-characteristic",
   "metadata": {},
   "source": [
    "So, looking both at the cross-validation and at the held-out test set, it appears that we get *slightly* worse accuracy with dirty data.\n",
    "\n",
    "### The actual fun stuff: what creates the error?\n",
    "\n",
    "I've saved the prediction probabilities from cross-validation on the training set (and from prediction on test), so that we can measure error for each chunk and try to diagnose the sources of error.\n",
    "\n",
    "Our strategy here involves three measurements.\n",
    "\n",
    "1. The \"divergence from clean\": how much does prediction error on a dirty chunk exceed (or fall below) the average error for clean chunks of this volume. Note that we cannot create a chunk-to-chunk mapping, since Hathi has not been trimmed.\n",
    "\n",
    "2. The \"worderrors\" for the chunk: how many words in the Hathi chunk were not found *either* in the Gutenberg text or in a simple dictionary of 10,000 common English words? This is not exactly the same as word-error-rate, but it's a close approximation that we can use for the moment.\n",
    "\n",
    "3. The \"passagefails\" for the chunk. What fraction of 20-character segments in the Hathi chunk could be found with fuzzy matching in (the whole) Gutenberg volume. We accepted difflib similarity scores of > 0.78 as a a match. This is **in effect** a measurement of the amount of text in the chunk that is substantially *different from* the Gutenberg text, because it's a table of contents, or introduction, or notes -- or a different translation!\n",
    "\n",
    "The question we're trying to answer: does accuracy fall mostly because of OCR errors, or because of paratext? As you'll see, it's difficult to answer with these corpora. This evidence casts some light, but we will also need more experiments with multiple corpora.\n",
    "\n",
    "We start by constructing vectors for real y and predicted $\\widehat{y}$ — for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "happy-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_y = dirtymodelmeta['authgender'].map({'f': 1, 'm': 0})  # we already put this in train-test order\n",
    "                                                            # so it will match the next line\n",
    "dirty_probabilities = np.append(dirtytrain_probabilities[ : , 0], dirtytest_probabilities[ : , 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-bobby",
   "metadata": {},
   "source": [
    "We can reason about two kinds of error. \n",
    "\n",
    "Directional error: did this chunk appear more negative or positive than it was in the real y vector.\n",
    "\n",
    "Absolute error: how far was this chunk from the real value in either direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "latin-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_error = dirty_y - dirty_probabilities\n",
    "dirty_absolute_error = np.abs(dirty_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-parcel",
   "metadata": {},
   "source": [
    "Repeat the above steps for the clean model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "tired-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_y = cleanmodelmeta['authgender'].map({'f': 1, 'm': 0})\n",
    "\n",
    "clean_probabilities = np.append(cleantrain_probabilities[ : , 0], cleantest_probabilities[ : , 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "prepared-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_error = clean_y - clean_probabilities\n",
    "clean_absolute_error = np.abs(clean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-short",
   "metadata": {},
   "source": [
    "#### Create mean values for clean volumes\n",
    "\n",
    "We create a dataframe and then groupby / average on Gutenberg indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "vocal-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gbi = [get_gbindex(x) for x in cleanmodelmeta['chunkid']]\n",
    "clean_df = pd.DataFrame({'chunkid': cleanmodelmeta['chunkid'], 'gbindex': clean_gbi,\n",
    "                         'clean_error': clean_error, 'clean_abs_error': clean_absolute_error})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "comparable-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "meandf = clean_df.groupby('gbindex').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "progressive-fellowship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_error</th>\n",
       "      <th>clean_abs_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbindex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39782</th>\n",
       "      <td>-0.039216</td>\n",
       "      <td>0.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34732</th>\n",
       "      <td>-0.000338</td>\n",
       "      <td>0.000338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50698</th>\n",
       "      <td>-0.533571</td>\n",
       "      <td>0.533571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27223</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DefoDLSfic</th>\n",
       "      <td>-0.128360</td>\n",
       "      <td>0.128360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52240</th>\n",
       "      <td>0.132031</td>\n",
       "      <td>0.132031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43358</th>\n",
       "      <td>0.170363</td>\n",
       "      <td>0.170363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37046</th>\n",
       "      <td>-0.000827</td>\n",
       "      <td>0.000827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51103</th>\n",
       "      <td>0.030277</td>\n",
       "      <td>0.030277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52072</th>\n",
       "      <td>-0.019937</td>\n",
       "      <td>0.019937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            clean_error  clean_abs_error\n",
       "gbindex                                 \n",
       "39782         -0.039216         0.039216\n",
       "34732         -0.000338         0.000338\n",
       "50698         -0.533571         0.533571\n",
       "27223          0.000028         0.000028\n",
       "DefoDLSfic    -0.128360         0.128360\n",
       "52240          0.132031         0.132031\n",
       "43358          0.170363         0.170363\n",
       "37046         -0.000827         0.000827\n",
       "51103          0.030277         0.030277\n",
       "52072         -0.019937         0.019937"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meandf.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "liberal-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_gbi = [get_gbindex(x) for x in dirtymodelmeta['chunkid']]\n",
    "dirty_df = pd.DataFrame({'chunkid': list(dirtymodelmeta['chunkid']), 'gbindex': dirty_gbi,\n",
    "                         'dirty_error': dirty_error, 'dirty_abs_error': dirty_absolute_error})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "fitting-toronto",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunkid</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>dirty_error</th>\n",
       "      <th>dirty_abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>31579_1</td>\n",
       "      <td>31579</td>\n",
       "      <td>-1.494654e-06</td>\n",
       "      <td>1.494654e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>36277_4</td>\n",
       "      <td>36277</td>\n",
       "      <td>-8.768491e-01</td>\n",
       "      <td>8.768491e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>32642_1</td>\n",
       "      <td>32642</td>\n",
       "      <td>-4.820049e-04</td>\n",
       "      <td>4.820049e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>49154_2</td>\n",
       "      <td>49154</td>\n",
       "      <td>-6.376934e-09</td>\n",
       "      <td>6.376934e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>amory.01_10</td>\n",
       "      <td>amory.01</td>\n",
       "      <td>-9.991067e-01</td>\n",
       "      <td>9.991067e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          chunkid   gbindex   dirty_error  dirty_abs_error\n",
       "449       31579_1     31579 -1.494654e-06     1.494654e-06\n",
       "1181      36277_4     36277 -8.768491e-01     8.768491e-01\n",
       "376       32642_1     32642 -4.820049e-04     4.820049e-04\n",
       "1406      49154_2     49154 -6.376934e-09     6.376934e-09\n",
       "33    amory.01_10  amory.01 -9.991067e-01     9.991067e-01"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "requested-conjunction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunkid</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>dirty_error</th>\n",
       "      <th>dirty_abs_error</th>\n",
       "      <th>clean_error</th>\n",
       "      <th>clean_abs_error</th>\n",
       "      <th>error_divergence</th>\n",
       "      <th>divergence_of_abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>40919_2</td>\n",
       "      <td>40919</td>\n",
       "      <td>-1.121937e-03</td>\n",
       "      <td>1.121937e-03</td>\n",
       "      <td>-5.572451e-04</td>\n",
       "      <td>5.572451e-04</td>\n",
       "      <td>-5.646914e-04</td>\n",
       "      <td>5.646914e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>33609_8</td>\n",
       "      <td>33609</td>\n",
       "      <td>-1.562672e-01</td>\n",
       "      <td>1.562672e-01</td>\n",
       "      <td>-1.443196e-01</td>\n",
       "      <td>1.443196e-01</td>\n",
       "      <td>-1.194759e-02</td>\n",
       "      <td>1.194759e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>49154_5</td>\n",
       "      <td>49154</td>\n",
       "      <td>-2.181895e-01</td>\n",
       "      <td>2.181895e-01</td>\n",
       "      <td>-1.994395e-06</td>\n",
       "      <td>1.994395e-06</td>\n",
       "      <td>-2.181876e-01</td>\n",
       "      <td>2.181876e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>34263_1</td>\n",
       "      <td>34263</td>\n",
       "      <td>-7.892935e-05</td>\n",
       "      <td>7.892935e-05</td>\n",
       "      <td>-2.545191e-01</td>\n",
       "      <td>2.545191e-01</td>\n",
       "      <td>2.544402e-01</td>\n",
       "      <td>-2.544402e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>34667_0</td>\n",
       "      <td>34667</td>\n",
       "      <td>-3.721030e-10</td>\n",
       "      <td>3.721030e-10</td>\n",
       "      <td>-6.780630e-08</td>\n",
       "      <td>6.780630e-08</td>\n",
       "      <td>6.743420e-08</td>\n",
       "      <td>-6.743420e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>63337_2</td>\n",
       "      <td>63337</td>\n",
       "      <td>1.274361e-03</td>\n",
       "      <td>1.274361e-03</td>\n",
       "      <td>4.424414e-01</td>\n",
       "      <td>4.424414e-01</td>\n",
       "      <td>-4.411670e-01</td>\n",
       "      <td>-4.411670e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>38049_5</td>\n",
       "      <td>38049</td>\n",
       "      <td>1.023892e-11</td>\n",
       "      <td>1.023892e-11</td>\n",
       "      <td>3.280943e-09</td>\n",
       "      <td>3.280943e-09</td>\n",
       "      <td>-3.270704e-09</td>\n",
       "      <td>-3.270704e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>45206_7</td>\n",
       "      <td>45206</td>\n",
       "      <td>-2.715326e-01</td>\n",
       "      <td>2.715326e-01</td>\n",
       "      <td>-1.464117e-01</td>\n",
       "      <td>1.464117e-01</td>\n",
       "      <td>-1.251209e-01</td>\n",
       "      <td>1.251209e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>54218_0</td>\n",
       "      <td>54218</td>\n",
       "      <td>8.952985e-06</td>\n",
       "      <td>8.952985e-06</td>\n",
       "      <td>7.142971e-02</td>\n",
       "      <td>7.142971e-02</td>\n",
       "      <td>-7.142076e-02</td>\n",
       "      <td>-7.142076e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>31835_2</td>\n",
       "      <td>31835</td>\n",
       "      <td>5.716124e-05</td>\n",
       "      <td>5.716124e-05</td>\n",
       "      <td>1.661478e-03</td>\n",
       "      <td>1.661478e-03</td>\n",
       "      <td>-1.604317e-03</td>\n",
       "      <td>-1.604317e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      chunkid gbindex   dirty_error  dirty_abs_error   clean_error  \\\n",
       "352   40919_2   40919 -1.121937e-03     1.121937e-03 -5.572451e-04   \n",
       "282   33609_8   33609 -1.562672e-01     1.562672e-01 -1.443196e-01   \n",
       "1401  49154_5   49154 -2.181895e-01     2.181895e-01 -1.994395e-06   \n",
       "1211  34263_1   34263 -7.892935e-05     7.892935e-05 -2.545191e-01   \n",
       "200   34667_0   34667 -3.721030e-10     3.721030e-10 -6.780630e-08   \n",
       "1509  63337_2   63337  1.274361e-03     1.274361e-03  4.424414e-01   \n",
       "162   38049_5   38049  1.023892e-11     1.023892e-11  3.280943e-09   \n",
       "1445  45206_7   45206 -2.715326e-01     2.715326e-01 -1.464117e-01   \n",
       "680   54218_0   54218  8.952985e-06     8.952985e-06  7.142971e-02   \n",
       "990   31835_2   31835  5.716124e-05     5.716124e-05  1.661478e-03   \n",
       "\n",
       "      clean_abs_error  error_divergence  divergence_of_abs_error  \n",
       "352      5.572451e-04     -5.646914e-04             5.646914e-04  \n",
       "282      1.443196e-01     -1.194759e-02             1.194759e-02  \n",
       "1401     1.994395e-06     -2.181876e-01             2.181876e-01  \n",
       "1211     2.545191e-01      2.544402e-01            -2.544402e-01  \n",
       "200      6.780630e-08      6.743420e-08            -6.743420e-08  \n",
       "1509     4.424414e-01     -4.411670e-01            -4.411670e-01  \n",
       "162      3.280943e-09     -3.270704e-09            -3.270704e-09  \n",
       "1445     1.464117e-01     -1.251209e-01             1.251209e-01  \n",
       "680      7.142971e-02     -7.142076e-02            -7.142076e-02  \n",
       "990      1.661478e-03     -1.604317e-03            -1.604317e-03  "
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty_df = dirty_df.merge(meandf, on = 'gbindex')\n",
    "dirty_df['error_divergence'] = dirty_df['dirty_error'] - dirty_df['clean_error']\n",
    "dirty_df['divergence_of_abs_error'] = dirty_df['dirty_abs_error'] - dirty_df['clean_abs_error']\n",
    "dirty_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "cooperative-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error0 = pd.read_csv('/Users/tunder/work/gh_align/hathichunkerrors.tsv', sep = '\\t')\n",
    "#error1 = pd.read_csv('/Users/tunder/work/gh_align/hathichunkerrs1.tsv', sep = '\\t')\n",
    "#error2 = pd.read_csv('/Users/tunder/work/gh_align/hathichunkerrs2.tsv', sep = '\\t')\n",
    "#error3 = pd.read_csv('/Users/tunder/work/gh_align/hathichunkerrs3.tsv', sep = '\\t')\n",
    "#error4 = pd.read_csv('/Users/tunder/work/gh_align/hathichunkerrs4.tsv', sep = '\\t')\n",
    "#error5 = pd.read_csv('/Users/tunder/work/gh_align/hathichunkerrs5.tsv', sep = '\\t')\n",
    "#chunkerrors = pd.concat([error0, error1, error2, error3, error4, error5], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "whole-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunkerrors.drop_duplicates(inplace = True)\n",
    "#print(chunkerrors.shape)\n",
    "#chunkerrors.to_csv('../metadata/hathi_chunk_errors.tsv', sep = '\\t', index = False)\n",
    "chunkerrors = pd.read_csv('../metadata/hathi_chunk_errors.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "romantic-estonia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunkid</th>\n",
       "      <th>passagefails</th>\n",
       "      <th>worderrors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36958_0</td>\n",
       "      <td>0.02087</td>\n",
       "      <td>0.02500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36958_1</td>\n",
       "      <td>0.02158</td>\n",
       "      <td>0.02177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36958_2</td>\n",
       "      <td>0.02039</td>\n",
       "      <td>0.02114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36958_3</td>\n",
       "      <td>0.02637</td>\n",
       "      <td>0.02103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49154_0</td>\n",
       "      <td>0.16478</td>\n",
       "      <td>0.01539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunkid  passagefails  worderrors\n",
       "0  36958_0       0.02087     0.02500\n",
       "1  36958_1       0.02158     0.02177\n",
       "2  36958_2       0.02039     0.02114\n",
       "3  36958_3       0.02637     0.02103\n",
       "4  49154_0       0.16478     0.01539"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkerrors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "structural-reynolds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunkid</th>\n",
       "      <th>passagefails</th>\n",
       "      <th>worderrors</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>dirty_error</th>\n",
       "      <th>dirty_abs_error</th>\n",
       "      <th>clean_error</th>\n",
       "      <th>clean_abs_error</th>\n",
       "      <th>error_divergence</th>\n",
       "      <th>divergence_of_abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36958_0</td>\n",
       "      <td>0.02087</td>\n",
       "      <td>0.02500</td>\n",
       "      <td>36958</td>\n",
       "      <td>-3.389505e-06</td>\n",
       "      <td>3.389505e-06</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36958_1</td>\n",
       "      <td>0.02158</td>\n",
       "      <td>0.02177</td>\n",
       "      <td>36958</td>\n",
       "      <td>-5.237897e-07</td>\n",
       "      <td>5.237897e-07</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>-0.000758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36958_2</td>\n",
       "      <td>0.02039</td>\n",
       "      <td>0.02114</td>\n",
       "      <td>36958</td>\n",
       "      <td>-5.590057e-05</td>\n",
       "      <td>5.590057e-05</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>-0.000703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36958_3</td>\n",
       "      <td>0.02637</td>\n",
       "      <td>0.02103</td>\n",
       "      <td>36958</td>\n",
       "      <td>-2.825321e-04</td>\n",
       "      <td>2.825321e-04</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49154_0</td>\n",
       "      <td>0.16478</td>\n",
       "      <td>0.01539</td>\n",
       "      <td>49154</td>\n",
       "      <td>-4.653712e-06</td>\n",
       "      <td>4.653712e-06</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunkid  passagefails  worderrors gbindex   dirty_error  dirty_abs_error  \\\n",
       "0  36958_0       0.02087     0.02500   36958 -3.389505e-06     3.389505e-06   \n",
       "1  36958_1       0.02158     0.02177   36958 -5.237897e-07     5.237897e-07   \n",
       "2  36958_2       0.02039     0.02114   36958 -5.590057e-05     5.590057e-05   \n",
       "3  36958_3       0.02637     0.02103   36958 -2.825321e-04     2.825321e-04   \n",
       "4  49154_0       0.16478     0.01539   49154 -4.653712e-06     4.653712e-06   \n",
       "\n",
       "   clean_error  clean_abs_error  error_divergence  divergence_of_abs_error  \n",
       "0    -0.000759         0.000759          0.000755                -0.000755  \n",
       "1    -0.000759         0.000759          0.000758                -0.000758  \n",
       "2    -0.000759         0.000759          0.000703                -0.000703  \n",
       "3    -0.000759         0.000759          0.000476                -0.000476  \n",
       "4    -0.000002         0.000002         -0.000003                 0.000003  "
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkerrors = chunkerrors.merge(dirty_df, on = 'chunkid', how = 'left')\n",
    "chunkerrors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "sublime-attachment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.08661333109432243, 0.00047530451176408295)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['error_divergence'], chunkerrors['divergence_of_abs_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "progressive-provincial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2610, 10)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkerrors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "stretch-essence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1624, 10)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkerrors=chunkerrors.dropna()\n",
    "chunkerrors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "textile-explosion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.00268935407583819, 0.9137619159860629)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['passagefails'], chunkerrors['error_divergence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "lesbian-group",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08875858123278027, 0.0003420361165436282)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['passagefails'], chunkerrors['divergence_of_abs_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "extreme-encounter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.05304554045242785, 0.03255449695433055)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['worderrors'], chunkerrors['error_divergence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "sophisticated-lying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08099315781022871, 0.001087748457765953)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['worderrors'], chunkerrors['divergence_of_abs_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-translation",
   "metadata": {},
   "source": [
    "What we see here is that the direction of the error has no correlation with paratext. But it does correlate negatively, and significantly, with OCR error. Greater OCR error consistently made writers seem more masculine. (If I haven't gotten my arithmetic reversed.)\n",
    "\n",
    "When we examine the divergence of absolute error (ignoring the direction), there's correlation in both cases. Paratext did significantly increase the absolute *amount* of error, and in fact did so more dramatically than OCR.\n",
    "\n",
    "We can also reason about the absolute value of the difference between clean and dirty. In other words, we could decide not to care whether error was increased or decreased, and instead care only about how *different* the amount of error is."
   ]
  },
  {
   "cell_type": "raw",
   "id": "virgin-power",
   "metadata": {},
   "source": [
    "pearsonr(chunkerrors['passagefails'], np.abs(chunkerrors['error_divergence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "hydraulic-metabolism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13237078740524869, 8.609634515224656e-08)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['worderrors'], np.abs(chunkerrors['error_divergence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-guess",
   "metadata": {},
   "source": [
    "That won't change if we substitute ```divergence_of_abs_error```.\n",
    "\n",
    "Finally, we want to think about the correlation of our predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "engaged-volunteer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.507774472802493, 3.684100007217292e-107)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['passagefails'], chunkerrors['worderrors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-fruit",
   "metadata": {},
   "source": [
    "**NOTE**: That's a very high correlation. Our predictve variables are extremely collinear.\n",
    "\n",
    "## Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "cooked-politics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               OLS Regression Results                              \n",
      "===================================================================================\n",
      "Dep. Variable:     divergence_of_abs_error   R-squared:                       0.010\n",
      "Model:                                 OLS   Adj. R-squared:                  0.008\n",
      "Method:                      Least Squares   F-statistic:                     7.870\n",
      "Date:                     Wed, 21 Apr 2021   Prob (F-statistic):           0.000397\n",
      "Time:                             08:48:49   Log-Likelihood:                -393.08\n",
      "No. Observations:                     1624   AIC:                             792.2\n",
      "Df Residuals:                         1621   BIC:                             808.3\n",
      "Df Model:                                2                                         \n",
      "Covariance Type:                 nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const           -0.0056      0.010     -0.581      0.561      -0.025       0.013\n",
      "passagefails     0.1163      0.052      2.237      0.025       0.014       0.218\n",
      "worderrors       0.2750      0.163      1.687      0.092      -0.045       0.595\n",
      "==============================================================================\n",
      "Omnibus:                      150.513   Durbin-Watson:                   1.551\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              276.844\n",
      "Skew:                           0.622   Prob(JB):                     7.66e-61\n",
      "Kurtosis:                       4.595   Cond. No.                         21.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "y = chunkerrors['divergence_of_abs_error']\n",
    "X = sm.add_constant(chunkerrors.loc[ : , ['passagefails', 'worderrors']])\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-separate",
   "metadata": {},
   "source": [
    "So, in short, when we consider both the gross inclusion of text that's not in Gutenberg (```passagefails```) and OCR quality, as measured by spelling errors in individual words (```worderrors```), only the former appears to be significantly distorting for this problem. When we look at them in isolation, ```worderrors``` are also significant. But when we combine all three variables in a single regression, most of the effect of ```worderrors``` can be accounted for by ```passagefails.```\n",
    "\n",
    "**Note:** we shouldn't put much weight on this at all. In this case the predictive variables are more highly correlated with each other than either really is with the response variable, so it's going to be hard to say which of the predictive variables is more important; collinearity is too slippery.\n",
    "\n",
    "But, for what it's worth, we get the opposite result with divergence of *directional* error. Here it's the OCR that matters, because it acts consistently to move texts toward the same pole of our gender binary.\n",
    "\n",
    "It will be interesting to see how this pattern changes with gender or date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "proud-harvey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       error_divergence   R-squared:                       0.004\n",
      "Model:                            OLS   Adj. R-squared:                  0.002\n",
      "Method:                 Least Squares   F-statistic:                     2.933\n",
      "Date:                Wed, 21 Apr 2021   Prob (F-statistic):             0.0535\n",
      "Time:                        08:49:46   Log-Likelihood:                -393.67\n",
      "No. Observations:                1624   AIC:                             793.3\n",
      "Df Residuals:                    1621   BIC:                             809.5\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const            0.0390      0.010      4.016      0.000       0.020       0.058\n",
      "passagefails     0.0591      0.052      1.135      0.256      -0.043       0.161\n",
      "worderrors      -0.3945      0.163     -2.420      0.016      -0.714      -0.075\n",
      "==============================================================================\n",
      "Omnibus:                       70.886   Durbin-Watson:                   1.542\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              223.979\n",
      "Skew:                          -0.038   Prob(JB):                     2.31e-49\n",
      "Kurtosis:                       4.818   Cond. No.                         21.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "y = chunkerrors['error_divergence']\n",
    "X = sm.add_constant(chunkerrors.loc[ : , ['passagefails', 'worderrors']])\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-narrow",
   "metadata": {},
   "source": [
    "There are several ways we could deal with the collinearity issue. One would be to construct a different algorithm for ```passagefails``` that separates it more rigorously from word-error-rate. For instance, when a passage is identified as *not the same* in both texts, I really shouldn't be checking the word-error rate for those words! That will make the variables less correlated.\n",
    "\n",
    "We may also get more leverage on the OCR / paratext question by considering alternate versions of the corpora. For instance, we still have a non-trimmed version of Gutenberg (which will partly preserve the paratext problem while eliminating the OCR problem), and we can construct a spelling-corrected version of Hathi (which will *partly* eliminate the OCR problem). That may give us a way to say whether paratext or OCR errors are a bigger distortion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-baking",
   "metadata": {},
   "source": [
    "Also, for this problem (gender prediction), none of the effects are really huge. Practically speaking, for downstream conclusions, none of the errors are a big thing to worry about. That may not hold equally true for genre and date prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-return",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
