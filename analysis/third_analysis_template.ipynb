{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "substantial-imagination",
   "metadata": {},
   "source": [
    "# A template for experiments\n",
    "\n",
    "I'm trying to develop a pipeline we can use for experiments on the NEH data: a template we can use for selecting data, training a model, and finally evaluating results. But this is by no means set in stone yet; it's a draft we can discuss and adjust.\n",
    "\n",
    "For a first test, let's consider the problem of author gender. We know our data model of gender is imperfect (limited for the most part to m/f), and we don't imagine a predictive model trained on this boundary will tell us very much about gender directly; it's almost certainly, to some degree, a proxy for genre. But it's a tricky boundary to model and thus a good place to start. We're in no danger of getting 100% accuracy!\n",
    "\n",
    "Our ultimate goal in this experiment, and other experiments based on this template, is to figure out:\n",
    "\n",
    "1. What kinds of *historical questions* are really distorted by the errors in digital libraries? Our working hypothesis is that certain boundaries (like genre and date) are likely to be more sensitive than others (like e.g. gender), because the errors are in practice not distributed randomly.\n",
    "\n",
    "2. What kinds of *errors* are most likely to produce distortion? I'm acting on the hypothesis (or hunch) that paratext is at least as big a problem as the better-studied problem of OCR error. For instance, the paratext in works of fiction is often from a different genre, and composed at a different time, than the original work. So it's very likely to distort conclusions.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hydraulic-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import rankdata\n",
    "from scipy.stats import pearsonr\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-runner",
   "metadata": {},
   "source": [
    "### Volume metadata\n",
    "\n",
    "This metadata file is in the GitHub repo. It has one row for each clean volume. \n",
    "\n",
    "Not all of these volumes have been trimmed and chunked; we will rely on the Box folders for a list of the chunks actually available to model. But then we will use this list in order to get features like genre, gender, and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accepting-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "volmeta = pd.read_csv('../metadata/updatedvolumemetadata.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rapid-study",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>author</th>\n",
       "      <th>authordate</th>\n",
       "      <th>title</th>\n",
       "      <th>latestcomp</th>\n",
       "      <th>hathidate</th>\n",
       "      <th>imprint</th>\n",
       "      <th>gutenstring</th>\n",
       "      <th>enumcron</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>...</th>\n",
       "      <th>contents</th>\n",
       "      <th>instances</th>\n",
       "      <th>genre</th>\n",
       "      <th>audience</th>\n",
       "      <th>authgender</th>\n",
       "      <th>multiplehtids</th>\n",
       "      <th>comments</th>\n",
       "      <th>coder</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Trimmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t5p851b8s</td>\n",
       "      <td>Reid, Stuart J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lord John Russell</td>\n",
       "      <td>1895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York;Harper &amp; brothers;1</td>\n",
       "      <td>Reid, Stuart J. | Lord John Russell</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27553</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hvd.32044070870779</td>\n",
       "      <td>Smiles, Samuel,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lives of the engineers</td>\n",
       "      <td>1879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London;J. Murray;1874-1877.</td>\n",
       "      <td>Smiles, Samuel | Lives of the Engineers</td>\n",
       "      <td>v. 5</td>\n",
       "      <td>27710</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio | short</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 people, mixed together (not one per chapter)</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdp.39015005892362</td>\n",
       "      <td>Cruttwell, Maud.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Luca Signorelli</td>\n",
       "      <td>1899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London;G. Bell &amp; sons;1899.</td>\n",
       "      <td>Cruttwell, Maud | Luca Signorelli</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>27759</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mdp.39015051108531</td>\n",
       "      <td>Bettany, George Thomas,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Life of Charles Darwin</td>\n",
       "      <td>1887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London;W. Scott;1887.</td>\n",
       "      <td>Bettany, George Thomas | Life of Charles Darwin</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>28380</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loc.ark+=13960=t6b27z54n</td>\n",
       "      <td>Gay, Sydney Howard,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>James Madison</td>\n",
       "      <td>1889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Boston;New York;Houghton, Mi</td>\n",
       "      <td>Gay, Sydney Howard | James Madison</td>\n",
       "      <td>&lt;blank&gt;</td>\n",
       "      <td>28992</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>morgan</td>\n",
       "      <td>gutenbiotrimmed</td>\n",
       "      <td>Trimmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docid                   author authordate  \\\n",
       "0  loc.ark+=13960=t5p851b8s          Reid, Stuart J.        NaN   \n",
       "1        hvd.32044070870779          Smiles, Samuel,        NaN   \n",
       "2        mdp.39015005892362         Cruttwell, Maud.        NaN   \n",
       "3        mdp.39015051108531  Bettany, George Thomas,        NaN   \n",
       "4  loc.ark+=13960=t6b27z54n      Gay, Sydney Howard,        NaN   \n",
       "\n",
       "                    title  latestcomp  hathidate  \\\n",
       "0       Lord John Russell        1895        NaN   \n",
       "1  Lives of the engineers        1879        NaN   \n",
       "2         Luca Signorelli        1899        NaN   \n",
       "3  Life of Charles Darwin        1887        NaN   \n",
       "4           James Madison        1889        NaN   \n",
       "\n",
       "                        imprint  \\\n",
       "0  New York;Harper & brothers;1   \n",
       "1   London;J. Murray;1874-1877.   \n",
       "2   London;G. Bell & sons;1899.   \n",
       "3         London;W. Scott;1887.   \n",
       "4  Boston;New York;Houghton, Mi   \n",
       "\n",
       "                                       gutenstring enumcron gbindex  ...  \\\n",
       "0              Reid, Stuart J. | Lord John Russell  <blank>   27553  ...   \n",
       "1          Smiles, Samuel | Lives of the Engineers     v. 5   27710  ...   \n",
       "2                Cruttwell, Maud | Luca Signorelli  <blank>   27759  ...   \n",
       "3  Bettany, George Thomas | Life of Charles Darwin  <blank>   28380  ...   \n",
       "4               Gay, Sydney Howard | James Madison  <blank>   28992  ...   \n",
       "\n",
       "   contents  instances        genre audience authgender  multiplehtids  \\\n",
       "0       NaN        NaN          bio      NaN          u            NaN   \n",
       "1       NaN        NaN  bio | short      NaN          m            NaN   \n",
       "2       NaN        NaN          bio      NaN          f            NaN   \n",
       "3       NaN        NaN          bio      NaN          m            NaN   \n",
       "4       NaN        NaN          bio      NaN          u            NaN   \n",
       "\n",
       "                                         comments   coder           Folder  \\\n",
       "0                                             NaN  morgan  gutenbiotrimmed   \n",
       "1  2 people, mixed together (not one per chapter)  morgan  gutenbiotrimmed   \n",
       "2                                             NaN  morgan  gutenbiotrimmed   \n",
       "3                                             NaN  morgan  gutenbiotrimmed   \n",
       "4                                             NaN  morgan  gutenbiotrimmed   \n",
       "\n",
       "   Trimmed  \n",
       "0  Trimmed  \n",
       "1  Trimmed  \n",
       "2  Trimmed  \n",
       "3  Trimmed  \n",
       "4  Trimmed  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volmeta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-hybrid",
   "metadata": {},
   "source": [
    "Here's a function we can use to generate a simpler genre column for modeling the biography / fiction boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prospective-hunter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly dutch\n"
     ]
    }
   ],
   "source": [
    "def simplify_genre(genrestring):\n",
    "    genres = [x.strip() for x in genrestring.split('|')]\n",
    "    if 'bio' in genres:\n",
    "        return 'bio'\n",
    "    elif 'fic' in genres:\n",
    "        return 'fic'\n",
    "    else:\n",
    "        print('anomaly', genrestring)\n",
    "        return float('nan')\n",
    "\n",
    "volmeta['simplegenre'] = volmeta['genre'].apply(simplify_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-throw",
   "metadata": {},
   "source": [
    "### Works we have actually trimmed and chunked so far\n",
    "\n",
    "The Box folders ```cleannarratives``` and ```dirtynarratives``` store processed texts.\n",
    "\n",
    "The ```cleannarratives``` folder consists of Gutenberg texts, manually trimmed and then automatically chunked to contain ~80,000 characters each.\n",
    "\n",
    "The ```dirtynarratives``` folder contains Hathi texts that have *not* been trimmed. The only clean-up done on these so far is that running headers have been removed. They've also been chunked to contain ~80,000 characters each, but the boundaries are not aligned with ```cleannarratives.```\n",
    "\n",
    "Note that the number of files will not be the same in these two folders, and the number of chunks for each volume will not be the same. They are chunked independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intelligent-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanfiles = [x for x in os.listdir('/Users/tunder/Box Sync/NEHproject/cleannarratives/')\n",
    "              if x.endswith('.txt')]\n",
    "dirtyfiles = [x for x in os.listdir('/Users/tunder/Box Sync/NEHproject/dirtynarratives/')\n",
    "               if x.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daily-concern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2489 clean chunks, and\n",
      "2610  dirty ones.\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \" + str(len(cleanfiles)) + \" clean chunks, and\")\n",
    "print(str(len(dirtyfiles)), \" dirty ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-ballet",
   "metadata": {},
   "source": [
    "What is actually in these data objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "raised-campus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'36965_3.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanfiles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-taste",
   "metadata": {},
   "source": [
    "How many *volumes* have we trimmed and chunked so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sacred-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gbindex(filename):\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "gbdict = dict()\n",
    "\n",
    "for filename in cleanfiles:\n",
    "    gbindex = get_gbindex(filename)\n",
    "    if gbindex not in gbdict:\n",
    "        gbdict[gbindex] = []\n",
    "    gbdict[gbindex].append(filename)\n",
    "\n",
    "gbset = set(gbdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bored-communication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 423 separate clean volumes that have been chunked.\n"
     ]
    }
   ],
   "source": [
    "print(\"We have \" + str(len(gbset)) + \" separate clean volumes that have been chunked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-instrumentation",
   "metadata": {},
   "source": [
    "Let's create a dataframe with just the volumes we're actually using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "assisted-difference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(423, 25)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ourtitles = volmeta.loc[volmeta['gbindex'].isin(gbset), : ]\n",
    "ourtitles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-baptist",
   "metadata": {},
   "source": [
    "### Balancing the distribution of classes across time\n",
    "\n",
    "Language change is very easy to model, so if you try to model the boundary between two categories that happen to be distributed differently across time (in your collection), you're very likely to get a model of language change. That's a problem if you want to study the categorical difference in itself, separated from confounding issues of chronology that might just be selection bias.\n",
    "\n",
    "Here we're distinguishing books written by men from those written by women. And although we don't really care about the model in its own right (since we're interested in the consequences of OCR distortion), it's still important to know what we're modeling, because OCR distortion could have *different* effects on different kinds of boundaries (e.g. chronological or demographic). \n",
    "\n",
    "So we need to be careful to balance the classes across time. At a minimum, we should require the median date for both categories to be roughly the same. A more ambitious approach would match the full distribution. But for right now let's keep it simple.\n",
    "\n",
    "Also, in order to keep things simple, let's select both classes to be the same size. So we'll match the size of the smaller class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compliant-egypt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smaller class has  139  volumes.\n"
     ]
    }
   ],
   "source": [
    "indexes_f = ourtitles.loc[ourtitles.authgender == 'f', : ].index.tolist()\n",
    "indexes_m = ourtitles.loc[ourtitles.authgender == 'm', : ].index.tolist()\n",
    "print('The smaller class has ', len(indexes_f), ' volumes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-method",
   "metadata": {},
   "source": [
    "The following function selects a matching number of volumes while keeping the median date similar in both categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "scenic-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_medians(smaller_indexes, larger_indexes, metadata):\n",
    "    '''\n",
    "    smaller_indexes = indexes of metadata for the smaller class\n",
    "    larger_indexes = indexes of metadata for the larger class\n",
    "    '''\n",
    "    selected_from_large = []\n",
    "    smaller_median = np.median(metadata.loc[smaller_indexes, 'latestcomp'])\n",
    "    \n",
    "    largerdf = metadata.loc[larger_indexes, : ]\n",
    "    above_median = largerdf.loc[largerdf['latestcomp'] >= smaller_median, : ].index.tolist()\n",
    "    below_median = largerdf.loc[largerdf['latestcomp'] <= smaller_median, : ].index.tolist()\n",
    "    \n",
    "    for i in range(len(smaller_indexes)):\n",
    "        if len(selected_from_large) > 0:\n",
    "            larger_median = np.median(metadata.loc[selected_from_large, 'latestcomp'])\n",
    "        else:\n",
    "            larger_median = smaller_median\n",
    "        \n",
    "        if larger_median >= smaller_median and len(below_median) > 0:\n",
    "            selected = random.sample(below_median, 1)[0]\n",
    "            below_median.pop(below_median.index(selected))\n",
    "            selected_from_large.append(selected)\n",
    "        elif larger_median <= smaller_median and len(above_median) > 0:\n",
    "            selected = random.sample(above_median, 1)[0]\n",
    "            above_median.pop(above_median.index(selected))\n",
    "            selected_from_large.append(selected)\n",
    "        else:\n",
    "            # we have no more items that won't distort the median\n",
    "            break\n",
    "\n",
    "    return selected_from_large, smaller_median, larger_median\n",
    "\n",
    "selected_m, median_f, median_m = match_medians(indexes_f, indexes_m, ourtitles)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "returning-portrait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1890.0 1889.0 139\n"
     ]
    }
   ],
   "source": [
    "print(median_f, median_m, len(selected_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "periodic-science",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_selected_vols = indexes_f + selected_m\n",
    "len(all_selected_vols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-demonstration",
   "metadata": {},
   "source": [
    "We've gathered the actual indexes of rows in ```ourtitles.``` Now let's convert those to Gutenberg indexes (or other clean-volume identifiers, in cases where volumes are drawn from ECCO, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "enhanced-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_gbindexes = ourtitles.loc[all_selected_vols, 'gbindex']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-polymer",
   "metadata": {},
   "source": [
    "### Merging chunk and volume\n",
    "\n",
    "So far we've been operating at the volume level. But we will need chunk-level metadata in order to actually model the files.\n",
    "\n",
    "Let's create dataframes that have a row for each chunk and merge those dataframes with the information in volmeta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "patient-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunkframe(filelist, volmeta):\n",
    "    \n",
    "    chunkids = []\n",
    "    gbindices = []\n",
    "    \n",
    "    for filename in filelist:\n",
    "        chunkids.append(filename.replace('.txt', ''))\n",
    "        gbindices.append(get_gbindex(filename))\n",
    "    \n",
    "    df = pd.DataFrame({'chunkid': chunkids, 'gbindex': gbindices})\n",
    "    \n",
    "    chunkmeta = df.merge(volmeta, how = 'inner', on = 'gbindex')\n",
    "    \n",
    "    return chunkmeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "coordinated-heaven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2489, 26)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanmeta = create_chunkframe(cleanfiles, ourtitles)\n",
    "cleanmeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "productive-simpson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2610, 26)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtymeta = create_chunkframe(dirtyfiles, ourtitles)\n",
    "dirtymeta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-advocate",
   "metadata": {},
   "source": [
    "Now we select the subsets of those dataframes with Gutenberg indexes that were selected by the match_medians function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ordinary-jungle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1560, 26)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanmodelmeta = cleanmeta.loc[cleanmeta['gbindex'].isin(selected_gbindexes), : ]\n",
    "cleanmodelmeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "advised-opening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1648, 26)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtymodelmeta = dirtymeta.loc[dirtymeta['gbindex'].isin(selected_gbindexes), : ]\n",
    "dirtymodelmeta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-blogger",
   "metadata": {},
   "source": [
    "### Term-document matrices for clean and dirty narratives \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rural-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rootdir = '/Users/tunder/Box Sync/NEHproject/cleannarratives/'\n",
    "\n",
    "clean_paths = []\n",
    "\n",
    "for chunk_id in cleanmodelmeta['chunkid']:\n",
    "    clean_paths.append(clean_rootdir + chunk_id + '.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "original-clothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>11th</th>\n",
       "      <th>12</th>\n",
       "      <th>12th</th>\n",
       "      <th>13</th>\n",
       "      <th>13th</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youths</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zealously</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zest</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chunkid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36965_3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 17000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         000  10  100  10th  11  11th  12  12th  13  13th  ...  youth  \\\n",
       "chunkid                                                    ...          \n",
       "36965_3    0   1    0     0   0     0   0     0   0     0  ...      1   \n",
       "36965_2    0   2    0     0   2     0   2     1   2     0  ...      0   \n",
       "36965_0    0   0    0     0   0     0   0     1   0     0  ...      9   \n",
       "36965_1    0   0    0     0   0     0   0     0   0     0  ...      3   \n",
       "36965_4    0   2    0     1   1     0   0     0   0     0  ...      5   \n",
       "\n",
       "         youthful  youths  zeal  zealous  zealously  zenith  zest  zigzag  \\\n",
       "chunkid                                                                     \n",
       "36965_3         0       0     0        0          0       0     0       0   \n",
       "36965_2         0       0     1        0          0       0     0       0   \n",
       "36965_0         2       0     0        0          0       0     0       0   \n",
       "36965_1         1       0     0        0          0       0     0       0   \n",
       "36965_4         0       0     0        0          0       0     0       0   \n",
       "\n",
       "         zone  \n",
       "chunkid        \n",
       "36965_3     0  \n",
       "36965_2     0  \n",
       "36965_0     0  \n",
       "36965_1     0  \n",
       "36965_4     0  \n",
       "\n",
       "[5 rows x 17000 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_model_chunk_ids = cleanmodelmeta['chunkid']\n",
    "\n",
    "vectorizer = CountVectorizer(input = 'filename', min_df = .02)\n",
    "sparse_clean_counts = vectorizer.fit_transform(clean_paths) # the vectorizer produces something\n",
    "                                                               # called a 'sparse matrix'; we need to\n",
    "                                                               # unpack it\n",
    "clean_wordcounts = pd.DataFrame(sparse_clean_counts.toarray(), index = clean_model_chunk_ids, \n",
    "                            columns = vectorizer.get_feature_names())\n",
    "clean_wordcounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "attended-court",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>...</th>\n",
       "      <th>ſwore</th>\n",
       "      <th>ſº</th>\n",
       "      <th>ﬁlled</th>\n",
       "      <th>ﬁnd</th>\n",
       "      <th>ﬁne</th>\n",
       "      <th>ﬁnished</th>\n",
       "      <th>ﬁre</th>\n",
       "      <th>ﬁrst</th>\n",
       "      <th>ﬁve</th>\n",
       "      <th>ﬁxed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chunkid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36965_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20019 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00  000  01  10  100  101  102  103  104  105  ...  ſwore  ſº  ﬁlled  \\\n",
       "chunkid                                                 ...                     \n",
       "36965_3   0    0   0   1    0    0    0    0    0    0  ...      0   0      0   \n",
       "36965_2   0    0   0   0    0    0    0    0    0    0  ...      0   0      0   \n",
       "36965_0   0    0   0   0    1    0    0    0    0    0  ...      0   0      0   \n",
       "36965_1   0    0   0   0    0    0    0    0    0    0  ...      0   0      0   \n",
       "36965_4   3    0   0   1    0    0    0    0    0    0  ...      0   0      0   \n",
       "\n",
       "         ﬁnd  ﬁne  ﬁnished  ﬁre  ﬁrst  ﬁve  ﬁxed  \n",
       "chunkid                                           \n",
       "36965_3    0    0        0    0     0    0     0  \n",
       "36965_2    0    0        0    0     0    0     0  \n",
       "36965_0    0    0        0    0     0    0     0  \n",
       "36965_1    0    0        0    0     0    0     0  \n",
       "36965_4    0    0        0    0     0    0     0  \n",
       "\n",
       "[5 rows x 20019 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty_rootdir = '/Users/tunder/Box Sync/NEHproject/dirtynarratives/'\n",
    "\n",
    "dirty_paths = []\n",
    "\n",
    "for chunk_id in dirtymodelmeta['chunkid']:\n",
    "    dirty_paths.append(dirty_rootdir + chunk_id + '.txt')\n",
    "\n",
    "dirty_model_chunk_ids = dirtymodelmeta['chunkid']\n",
    "    \n",
    "vectorizer = CountVectorizer(input = 'filename', min_df = .02)\n",
    "sparse_dirty_counts = vectorizer.fit_transform(dirty_paths) # the vectorizer produces something\n",
    "                                                               # called a 'sparse matrix'; we need to\n",
    "                                                               # unpack it\n",
    "dirty_wordcounts = pd.DataFrame(sparse_dirty_counts.toarray(), index = dirty_model_chunk_ids, \n",
    "                            columns = vectorizer.get_feature_names())\n",
    "dirty_wordcounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-environment",
   "metadata": {},
   "source": [
    "### Turn wordcounts into normalized frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "turkish-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rowsums = clean_wordcounts.sum(axis = 'columns')\n",
    "clean_freqs = clean_wordcounts.divide(clean_rowsums, axis = 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "recorded-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_rowsums = dirty_wordcounts.sum(axis = 'columns')\n",
    "dirty_freqs = dirty_wordcounts.divide(dirty_rowsums, axis = 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "chubby-reduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>...</th>\n",
       "      <th>ſwore</th>\n",
       "      <th>ſº</th>\n",
       "      <th>ﬁlled</th>\n",
       "      <th>ﬁnd</th>\n",
       "      <th>ﬁne</th>\n",
       "      <th>ﬁnished</th>\n",
       "      <th>ﬁre</th>\n",
       "      <th>ﬁrst</th>\n",
       "      <th>ﬁve</th>\n",
       "      <th>ﬁxed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chunkid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36965_3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36965_4</th>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20019 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               00  000   01        10       100  101  102  103  104  105  ...  \\\n",
       "chunkid                                                                   ...   \n",
       "36965_3  0.000000  0.0  0.0  0.000074  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "36965_2  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "36965_0  0.000000  0.0  0.0  0.000000  0.000073  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "36965_1  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "36965_4  0.000216  0.0  0.0  0.000072  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "         ſwore   ſº  ﬁlled  ﬁnd  ﬁne  ﬁnished  ﬁre  ﬁrst  ﬁve  ﬁxed  \n",
       "chunkid                                                              \n",
       "36965_3    0.0  0.0    0.0  0.0  0.0      0.0  0.0   0.0  0.0   0.0  \n",
       "36965_2    0.0  0.0    0.0  0.0  0.0      0.0  0.0   0.0  0.0   0.0  \n",
       "36965_0    0.0  0.0    0.0  0.0  0.0      0.0  0.0   0.0  0.0   0.0  \n",
       "36965_1    0.0  0.0    0.0  0.0  0.0      0.0  0.0   0.0  0.0   0.0  \n",
       "36965_4    0.0  0.0    0.0  0.0  0.0      0.0  0.0   0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 20019 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty_freqs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-container",
   "metadata": {},
   "source": [
    "Notice the eighteenth-century long S's!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-closure",
   "metadata": {},
   "source": [
    "### Separate train & validation from final test set\n",
    "\n",
    "We will optimize our model by cross-validating on 3/4 of the data, and finally test on a held-out 1/4. The same volumes should be held out for both clean and dirty data. Note that we select *by author* to avoid leakage across this boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "impressive-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_authors = list(set(volmeta.loc[volmeta['gbindex'].isin(selected_gbindexes), 'author']))\n",
    "random.shuffle(selected_authors)\n",
    "threequarters = int(len(selected_authors) * .75)\n",
    "trainauthors = selected_authors[0 : threequarters]\n",
    "testauthors = selected_authors[threequarters: ]\n",
    "\n",
    "cleantrain = cleanmodelmeta.loc[cleanmodelmeta['author'].isin(trainauthors), : ]\n",
    "cleantest = cleanmodelmeta.loc[cleanmodelmeta['author'].isin(testauthors), : ]\n",
    "cleantrain_freqs = clean_freqs.loc[cleantrain.chunkid]\n",
    "cleantest_freqs = clean_freqs.loc[cleantest.chunkid]\n",
    "\n",
    "dirtytrain = dirtymodelmeta.loc[dirtymodelmeta['author'].isin(trainauthors), : ]\n",
    "dirtytest = dirtymodelmeta.loc[dirtymodelmeta['author'].isin(testauthors), : ]\n",
    "dirtytrain_freqs = dirty_freqs.loc[dirtytrain.chunkid]\n",
    "dirtytest_freqs = dirty_freqs.loc[dirtytest.chunkid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "married-fantasy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean training set:  (1231, 26)\n",
      "Clean test set:  (329, 26)\n",
      "Hathi training set:  (1302, 26)\n",
      "Hathi testing set:  (346, 26)\n"
     ]
    }
   ],
   "source": [
    "print('Clean training set: ', cleantrain.shape)\n",
    "print('Clean test set: ', cleantest.shape)\n",
    "print('Hathi training set: ', dirtytrain.shape)\n",
    "print('Hathi testing set: ', dirtytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-waste",
   "metadata": {},
   "source": [
    "Let's put our master modelmeta frames in the right order to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "brown-block",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1648, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1648, 26)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtymodelmeta.set_index('chunkid', inplace = True)\n",
    "print(dirtymodelmeta.shape)\n",
    "dirty_ids_order = np.append(list(dirtytrain.chunkid), list(dirtytest.chunkid))\n",
    "dirtymodelmeta = dirtymodelmeta.loc[dirty_ids_order, : ]\n",
    "dirtymodelmeta.reset_index(inplace = True)\n",
    "dirtymodelmeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "economic-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1560, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1560, 26)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanmodelmeta.set_index('chunkid', inplace = True)\n",
    "print(cleanmodelmeta.shape)\n",
    "clean_ids_order = np.append(list(cleantrain.chunkid), list(cleantest.chunkid))\n",
    "cleanmodelmeta = cleanmodelmeta.loc[clean_ids_order, : ]\n",
    "cleanmodelmeta.reset_index(inplace = True)\n",
    "cleanmodelmeta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-coordination",
   "metadata": {},
   "source": [
    "### Let's produce a model for the clean counts\n",
    "\n",
    "We're going to do a grid search for the best model. The outer loop will select the number of features. The inner loop will select the regularization constant.\n",
    "\n",
    "In selecting the top *n* features, we will always select the *n* with top *document* frequency.\n",
    "\n",
    "We will do the cross-validation only on the training set. This means in effect that there are lots of different \"validation sets\" inside the training set. But the test set itself is a separate issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "checked-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_freqs(X, y):\n",
    "    return np.count_nonzero(X, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "compact-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 10000 0.7148177288224495\n",
      "1000 3000 0.7123787044322057\n",
      "1000 1000 0.7042486231313927\n",
      "1000 500 0.7034356150013114\n",
      "1000 100 0.7058746393915553\n",
      "1000 10 0.7091332284290586\n",
      "1000 1 0.7099396800419616\n",
      "1000 0.1 0.7156241804353527\n",
      "1000 0.01 0.7391883031733543\n",
      "1000 0.001 0.7473183844741673\n",
      "1000 0.0001 0.7051075268817205\n",
      "1500 10000 0.7278455284552845\n",
      "1500 3000 0.728658536585366\n",
      "1500 1000 0.7270325203252033\n",
      "1500 500 0.7254130605822187\n",
      "1500 100 0.7254130605822188\n",
      "1500 10 0.7270325203252033\n",
      "1500 1 0.7327235772357723\n",
      "1500 0.1 0.7367886178861789\n",
      "1500 0.01 0.7465184893784422\n",
      "1500 0.001 0.7570679255179649\n",
      "1500 0.0001 0.7213414634146342\n",
      "2000 10000 0.7148373983739837\n",
      "2000 3000 0.7172698662470495\n",
      "2000 1000 0.7132179386309991\n",
      "2000 500 0.7115984788880148\n",
      "2000 100 0.7116050354051928\n",
      "2000 10 0.7148439548911617\n",
      "2000 1 0.7189024390243903\n",
      "2000 0.1 0.7294584316810909\n",
      "2000 0.01 0.7383884080776292\n",
      "2000 0.001 0.7578743771308682\n",
      "2000 0.0001 0.7237608182533439\n",
      "2500 10000 0.7238001573564122\n",
      "2500 3000 0.7286519800681879\n",
      "2500 1000 0.7270259638080252\n",
      "2500 500 0.7262195121951219\n",
      "2500 100 0.7262129556779439\n",
      "2500 10 0.7278324154209284\n",
      "2500 1 0.7302714398111724\n",
      "2500 0.1 0.7335234723314976\n",
      "2500 0.01 0.7424534487280356\n",
      "2500 0.001 0.7505769735116706\n",
      "2500 0.0001 0.7221413585103593\n",
      "3000 10000 0.7262588512981905\n",
      "3000 3000 0.7302976658798846\n",
      "3000 1000 0.722987149226331\n",
      "3000 500 0.7213611329661684\n",
      "3000 100 0.7238067138735904\n",
      "3000 10 0.7262457382638343\n",
      "3000 1 0.7278717545239969\n",
      "3000 0.1 0.7311106740099659\n",
      "3000 0.01 0.7384080776291634\n",
      "3000 0.001 0.7473314975085235\n",
      "3000 0.0001 0.7213414634146342\n",
      "3500 10000 0.7286782061369002\n",
      "3500 3000 0.7367951744033568\n",
      "3500 1000 0.7351626016260163\n",
      "3500 500 0.7351560451088383\n",
      "3500 100 0.7384015211119854\n",
      "3500 10 0.7392145292420667\n",
      "3500 1 0.7408471020194073\n",
      "3500 0.1 0.7440991345397324\n",
      "3500 0.01 0.7457317073170732\n",
      "3500 0.001 0.7489771833202203\n",
      "3500 0.0001 0.7237870443220562\n",
      "4000 10000 0.7278455284552845\n",
      "4000 3000 0.7351560451088383\n",
      "4000 1000 0.7432795698924731\n",
      "4000 500 0.7408405455022292\n",
      "4000 100 0.7432795698924732\n",
      "4000 10 0.7424665617623918\n",
      "4000 1 0.7449055861526357\n",
      "4000 0.1 0.7481445056386048\n",
      "4000 0.01 0.7473380540257015\n",
      "4000 0.001 0.7441056910569106\n",
      "4000 0.0001 0.7253999475478625\n",
      "4500 10000 0.7213545764489903\n",
      "4500 3000 0.7318908995541569\n",
      "4500 1000 0.729458431681091\n",
      "4500 500 0.7302714398111723\n",
      "4500 100 0.7302779963283504\n",
      "4500 10 0.731904012588513\n",
      "4500 1 0.735142932074482\n",
      "4500 0.1 0.7375819564647259\n",
      "4500 0.01 0.743266456858117\n",
      "4500 0.001 0.7424403356936795\n",
      "4500 0.0001 0.7245803829006032\n",
      "5000 10000 0.7205087857330186\n",
      "5000 3000 0.7286454235510097\n",
      "5000 1000 0.7367689483346447\n",
      "5000 500 0.7359559402045633\n",
      "5000 100 0.7383949645948072\n",
      "5000 10 0.7392014162077104\n",
      "5000 1 0.7383949645948072\n",
      "5000 0.1 0.7400209808549698\n",
      "5000 0.01 0.7367755048518226\n",
      "5000 0.001 0.737588512981904\n",
      "5000 0.0001 0.7253999475478625\n",
      "5500 10000 0.7205218987673747\n",
      "5500 3000 0.7278389719381064\n",
      "5500 1000 0.7310844479412536\n",
      "5500 500 0.7310778914240754\n",
      "5500 100 0.7318908995541568\n",
      "5500 10 0.7327039076842382\n",
      "5500 1 0.7327039076842382\n",
      "5500 0.1 0.7310844479412536\n",
      "5500 0.01 0.7392145292420667\n",
      "5500 0.001 0.7351363755573039\n",
      "5500 0.0001 0.7245869394177813\n",
      "6000 10000 0.7213676894833465\n",
      "6000 3000 0.7286847626540782\n",
      "6000 1000 0.7327432467873065\n",
      "6000 500 0.7303042223970627\n",
      "6000 100 0.7286716496197221\n",
      "6000 10 0.7327301337529505\n",
      "6000 1 0.7343495934959351\n",
      "6000 0.1 0.7351691581431944\n",
      "6000 0.01 0.7408536585365855\n",
      "6000 0.001 0.7351494885916601\n",
      "6000 0.0001 0.7221348019931813\n",
      "7000 10000 0.7367886178861789\n",
      "7000 3000 0.7392210857592446\n",
      "7000 1000 0.7432861264096512\n",
      "7000 500 0.7408471020194074\n",
      "7000 100 0.7408471020194074\n",
      "7000 10 0.7384080776291635\n",
      "7000 1 0.7392210857592447\n",
      "7000 0.1 0.7416601101494886\n",
      "7000 0.01 0.7440991345397325\n",
      "7000 0.001 0.7408602150537634\n",
      "7000 0.0001 0.7286388670338317\n"
     ]
    }
   ],
   "source": [
    "resultarray = []\n",
    "\n",
    "featureoptions = [1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 7000]\n",
    "c_options = [10000, 3000, 1000, 500, 100, 10, 1, .1, .01, .001, .0001]\n",
    "\n",
    "for featurecount in featureoptions:\n",
    "    docfreqs = []\n",
    "    for col in cleantrain_freqs.columns:\n",
    "        docfreqs.append((sum(cleantrain_freqs[col] > 0), col))\n",
    "    docfreqs.sort()\n",
    "    features = [x[1] for x in docfreqs[-featurecount : ]] # because sorted in ascending order\n",
    "    \n",
    "    model_features = cleantrain_freqs.loc[ : , features]\n",
    "    \n",
    "    resultrow = []\n",
    "    \n",
    "    for c_param in c_options:\n",
    "        logreg = LogisticRegression(C = c_param, max_iter = 2000)\n",
    "        scaler = StandardScaler()\n",
    "        # feature_selector = SelectKBest(get_doc_freqs, k = featurecount)\n",
    "        pipe = Pipeline([\n",
    "            # ('fkb', feature_selector),\n",
    "            ('sc', scaler),\n",
    "            ('lr', logreg)\n",
    "        ])    \n",
    "        grouper = GroupKFold(n_splits = 10)\n",
    "        cv_results = cross_validate(estimator = pipe, \n",
    "                                    X = model_features,\n",
    "                                    y = cleantrain['authgender'], \n",
    "                                    groups = cleantrain['author'], \n",
    "                                    cv = grouper)\n",
    "        mean_score = np.mean(cv_results['test_score'])\n",
    "        print(featurecount, c_param, mean_score)\n",
    "        resultrow.append(mean_score)\n",
    "    \n",
    "    resultarray.append(resultrow)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-dictionary",
   "metadata": {},
   "source": [
    "#### visualize the grid search\n",
    "\n",
    "This can be a good way to check that you've really covered the space and \"surrounded\" the optimum value on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "lucky-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultarray = np.array(resultarray)\n",
    "thearrayshape = resultarray.shape\n",
    "resarray = rankdata(resultarray).reshape(thearrayshape)   # I'm doing this because otherwise\n",
    "                                                    # fine details at top of range can be hard to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "flying-palestinian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.imshow(resarray, cmap='hot', aspect = 'auto')\n",
    "plt.yticks(ticks = [x for x in range(len(featureoptions))], labels = featureoptions)\n",
    "thexlabels = [str(x) for x in c_options]\n",
    "thexlabels[0] = '10^4'\n",
    "plt.xticks(ticks = [x for x in range(len(c_options))], labels = thexlabels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "appropriate-decade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 0.001\n"
     ]
    }
   ],
   "source": [
    "maxtuple = np.where(resarray == np.amax(resarray))\n",
    "print(featureoptions[maxtuple[0][0]], c_options[maxtuple[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "collect-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "docfreqs = []\n",
    "for col in clean_wordcounts.columns:\n",
    "    docfreqs.append((sum(clean_wordcounts[col] > 0), col))\n",
    "docfreqs.sort()\n",
    "features = [x[1] for x in docfreqs[-2000: ]] #because sorted ascending\n",
    "\n",
    "train_features = cleantrain_freqs.loc[ : , features]  \n",
    "    \n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "bestmodel = LogisticRegression(C = .001, max_iter = 2000)\n",
    "cleantrain_probabilities = cross_val_predict(bestmodel, train_features, cleantrain['authgender'], \n",
    "                                     groups = cleantrain['author'], cv = grouper,\n",
    "                                    method = 'predict_proba')\n",
    "\n",
    "## NOW APPLY THE SAME SCALER AND MODEL TO TEST SET\n",
    "\n",
    "bestmodel = LogisticRegression(C = .001, max_iter = 2000)\n",
    "bestmodel.fit(train_features, cleantrain['authgender'])\n",
    "\n",
    "test_features = cleantest_freqs.loc[ : , features] \n",
    "test_features = scaler.transform(test_features) # Note this is the same scaler we fit\n",
    "                                                # to train features; we DON'T fit a new one to the\n",
    "                                                # test features. We deliberately blind ourselves to that\n",
    "                                                # information.\n",
    "\n",
    "cleantest_predictions = bestmodel.predict(test_features)\n",
    "cleantest_probabilities = bestmodel.predict_proba(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "integral-applicant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8145896656534954"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cleantest_predictions == cleantest['authgender']) / len(cleantest['authgender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-remains",
   "metadata": {},
   "source": [
    "### Now a model for the dirty counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "alternate-paris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 100000 0.6864322877548513\n",
      "1000 10000 0.6955866300691438\n",
      "1000 3000 0.6993739729706355\n",
      "1000 1000 0.703202601883589\n",
      "1000 500 0.7016700123357322\n",
      "1000 100 0.7001314597586568\n",
      "1000 10 0.6970720616149341\n",
      "1000 1 0.7001430216626383\n",
      "1000 0.1 0.7039479805359444\n",
      "1000 0.01 0.7070370117485332\n",
      "1000 0.001 0.7147413365378925\n",
      "1000 0.0001 0.6708931798422301\n",
      "1500 100000 0.6948711576014966\n",
      "1500 10000 0.701019222803352\n",
      "1500 3000 0.7017292784734644\n",
      "1500 1000 0.7048123466568346\n",
      "1500 500 0.7063449362046913\n",
      "1500 100 0.7071083860219313\n",
      "1500 10 0.7124694224055134\n",
      "1500 1 0.711682484625854\n",
      "1500 0.1 0.7170790260689072\n",
      "1500 0.01 0.7309137090496934\n",
      "1500 0.001 0.733937511095331\n",
      "1500 0.0001 0.6839584499765575\n",
      "2000 100000 0.7125580484962697\n",
      "2000 10000 0.7248661049584182\n",
      "2000 3000 0.7294520385821647\n",
      "2000 1000 0.730203562340967\n",
      "2000 500 0.7286650097638915\n",
      "2000 100 0.7317420238794285\n",
      "2000 10 0.7301975993117481\n",
      "2000 1 0.7363456645136035\n",
      "2000 0.1 0.7401920004369854\n",
      "2000 0.01 0.7409614132834441\n",
      "2000 0.001 0.7347483009918657\n",
      "2000 0.0001 0.6901539462963215\n",
      "2500 100000 0.7086709727931103\n",
      "2500 10000 0.715570743830996\n",
      "2500 3000 0.7286007820216944\n",
      "2500 1000 0.7270858084456522\n",
      "2500 500 0.7271035154560808\n",
      "2500 100 0.7293994637825633\n",
      "2500 10 0.728618397993509\n",
      "2500 1 0.7278373322044545\n",
      "2500 0.1 0.7278372411658404\n",
      "2500 0.01 0.7301450245121468\n",
      "2500 0.001 0.7354646838456531\n",
      "2500 0.0001 0.687858544201523\n",
      "3000 100000 0.6895222293535803\n",
      "3000 10000 0.6987296926991584\n",
      "3000 3000 0.7102093432929577\n",
      "3000 1000 0.712487493570398\n",
      "3000 500 0.7148070208979138\n",
      "3000 100 0.7148128928885187\n",
      "3000 10 0.7132803943792759\n",
      "3000 1 0.7117478958700333\n",
      "3000 0.1 0.7109726110329697\n",
      "3000 0.01 0.711718171762553\n",
      "3000 0.001 0.7224406086841733\n",
      "3000 0.0001 0.6893795718453982\n",
      "3500 100000 0.7063986034676607\n",
      "3500 10000 0.7063927314770561\n",
      "3500 3000 0.7071265482254299\n",
      "3500 1000 0.7070732451169163\n",
      "3500 500 0.7070791171075211\n",
      "3500 100 0.7093928634830463\n",
      "3500 10 0.7070910431659588\n",
      "3500 1 0.710162094252277\n",
      "3500 0.1 0.7132213103187717\n",
      "3500 0.01 0.7154995516348259\n",
      "3500 0.001 0.7139491640379266\n",
      "3500 0.0001 0.6909651913859264\n",
      "4000 100000 0.701061237123726\n",
      "4000 10000 0.7018305589315708\n",
      "4000 3000 0.7009721558398995\n",
      "4000 1000 0.7024870383773277\n",
      "4000 500 0.7024929103679326\n",
      "4000 100 0.7024869473387138\n",
      "4000 10 0.7047828046265823\n",
      "4000 1 0.7062976871640105\n",
      "4000 0.1 0.7093747923181618\n",
      "4000 0.01 0.7139901769335464\n",
      "4000 0.001 0.7139838497498714\n",
      "4000 0.0001 0.6825032887699318\n",
      "4500 100000 0.6864691583935326\n",
      "4500 10000 0.6902858612480484\n",
      "4500 3000 0.6948184917632815\n",
      "4500 1000 0.6978538557129006\n",
      "4500 500 0.6955520353958131\n",
      "4500 100 0.6993804822315386\n",
      "4500 10 0.6986112514623077\n",
      "4500 1 0.7009130717793952\n",
      "4500 0.1 0.7039782508751086\n",
      "4500 0.01 0.7131855321434586\n",
      "4500 0.001 0.7108658227387147\n",
      "4500 0.0001 0.686373294732961\n",
      "5000 100000 0.681817995602835\n",
      "5000 10000 0.6902443931593585\n",
      "5000 3000 0.694037425974227\n",
      "5000 1000 0.6955460723665943\n",
      "5000 500 0.6932086559514218\n",
      "5000 100 0.6932146189806406\n",
      "5000 10 0.6955046042779045\n",
      "5000 1 0.6962679630565305\n",
      "5000 0.1 0.7000965919694839\n",
      "5000 0.01 0.7024042842771763\n",
      "5000 0.001 0.7039485267676284\n",
      "5000 0.0001 0.6932906817426612\n",
      "5500 100000 0.6864100743330284\n",
      "5500 10000 0.6894457113984896\n",
      "5500 3000 0.6909664659265228\n",
      "5500 1000 0.6978717448005571\n",
      "5500 500 0.6970670089718554\n",
      "5500 100 0.6970787529530651\n",
      "5500 10 0.6986113425009217\n",
      "5500 1 0.6939958668469232\n",
      "5500 0.1 0.6924514422792428\n",
      "5500 0.01 0.6970491198841988\n",
      "5500 0.001 0.7008835297491431\n",
      "5500 0.0001 0.6817520381269716\n",
      "6000 100000 0.6794808523035045\n",
      "6000 10000 0.6863803502255481\n",
      "6000 3000 0.6863625521765055\n",
      "6000 1000 0.6909246336833769\n",
      "6000 500 0.6924512602020146\n",
      "6000 100 0.6940015567603\n",
      "6000 10 0.6909245426447627\n",
      "6000 1 0.6924452971727958\n",
      "6000 0.1 0.6916819383941699\n",
      "6000 0.01 0.6901551297983041\n",
      "6000 0.001 0.6932379248658319\n",
      "6000 0.0001 0.6763966916567663\n",
      "6500 100000 0.6795099391406866\n",
      "6500 10000 0.6795039761114677\n",
      "6500 3000 0.6825571381101294\n",
      "6500 1000 0.6871313277526663\n",
      "6500 500 0.6871430717338758\n",
      "6500 100 0.6871371087046569\n",
      "6500 10 0.6879063394738878\n",
      "6500 1 0.6863559518769885\n",
      "6500 0.1 0.689427002963307\n",
      "6500 0.01 0.6886460282128665\n",
      "6500 0.001 0.6878884503862314\n",
      "6500 0.0001 0.6771421613477356\n",
      "7000 100000 0.6763560429156026\n",
      "7000 10000 0.6809598656270057\n",
      "7000 3000 0.6855754323196184\n",
      "7000 1000 0.6931910399796073\n",
      "7000 500 0.6939543077196192\n",
      "7000 100 0.6954986412486857\n",
      "7000 10 0.6970371938257611\n",
      "7000 1 0.6947591345869351\n",
      "7000 0.1 0.6985876724612743\n",
      "7000 0.01 0.6993391051814627\n",
      "7000 0.001 0.695516166181886\n",
      "7000 0.0001 0.687101148452116\n"
     ]
    }
   ],
   "source": [
    "resultarray = []\n",
    "\n",
    "featureoptions = [1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000]\n",
    "c_options = [100000, 10000, 3000, 1000, 500, 100, 10, 1, .1, .01, .001, .0001]\n",
    "\n",
    "for featurecount in featureoptions:\n",
    "    docfreqs = []\n",
    "    for col in dirtytrain_freqs.columns:\n",
    "        docfreqs.append((sum(dirtytrain_freqs[col] > 0), col))\n",
    "    docfreqs.sort()\n",
    "    features = [x[1] for x in docfreqs[-featurecount: ]]  # because sorted ascending\n",
    "    \n",
    "    model_features = dirtytrain_freqs.loc[ : , features]\n",
    "    \n",
    "    resultrow = []\n",
    "    \n",
    "    for c_param in c_options:\n",
    "        logreg = LogisticRegression(C = c_param, max_iter = 2000)\n",
    "        scaler = StandardScaler()\n",
    "        # feature_selector = SelectKBest(get_doc_freqs, k = featurecount)\n",
    "        pipe = Pipeline([\n",
    "            # ('fs', feature_selector),\n",
    "            ('sc', scaler),\n",
    "            ('lr', logreg)\n",
    "        ])\n",
    "        grouper = GroupKFold(n_splits = 10)\n",
    "        cv_results = cross_validate(estimator = pipe, \n",
    "                                    X = model_features, \n",
    "                                    y = dirtytrain['authgender'], \n",
    "                                    groups = dirtytrain['author'], \n",
    "                                    cv = grouper)\n",
    "        mean_score = np.mean(cv_results['test_score'])\n",
    "        print(featurecount, c_param, mean_score)\n",
    "        resultrow.append(mean_score)\n",
    "    \n",
    "    resultarray.append(resultrow)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "liable-petroleum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultarray = np.array(resultarray)\n",
    "thearrayshape = resultarray.shape\n",
    "resarray = rankdata(resultarray).reshape(thearrayshape)   # I'm doing this because otherwise\n",
    "                                                    # fine details at top of range can be hard to see.\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax.imshow(resarray, cmap='hot', aspect = 'auto')\n",
    "plt.yticks(ticks = [x for x in range(len(featureoptions))], labels = featureoptions)\n",
    "thexlabels = [str(x) for x in c_options]\n",
    "thexlabels[0] = '10^5'\n",
    "thexlabels[1] = '10^4'\n",
    "plt.xticks(ticks = [x for x in range(len(c_options))], labels = thexlabels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "upset-accent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 0.01\n"
     ]
    }
   ],
   "source": [
    "maxtuple = np.where(resarray == np.amax(resarray))\n",
    "print(featureoptions[maxtuple[0][0]], c_options[maxtuple[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "floral-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "docfreqs = []\n",
    "for col in dirtytrain_freqs.columns:\n",
    "    docfreqs.append((sum(dirtytrain_freqs[col] > 0), col))\n",
    "docfreqs.sort()\n",
    "features = [x[1] for x in docfreqs[-2000 : ]]\n",
    "\n",
    "train_features = dirtytrain_freqs.loc[ : , features]\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "bestmodel = LogisticRegression(C = .01, max_iter = 2000)\n",
    "dirtytrain_probabilities = cross_val_predict(bestmodel, train_features, dirtytrain['authgender'], \n",
    "                                     groups = dirtytrain['author'], cv = grouper,\n",
    "                                    method = 'predict_proba')\n",
    "\n",
    "## NOW APPLY THE SAME SCALER AND MODEL TO TEST SET\n",
    "\n",
    "bestmodel = LogisticRegression(C = .01, max_iter = 2000)\n",
    "bestmodel.fit(train_features, dirtytrain['authgender'])\n",
    "\n",
    "test_features = dirtytest_freqs.loc[ : , features] \n",
    "test_features = scaler.transform(test_features)    # Note this is the same scaler we fit\n",
    "                                                # to train features; we DON'T fit a new one to the\n",
    "                                                # test features. We deliberately blind ourselves to that\n",
    "                                                # information.\n",
    "\n",
    "dirtytest_predictions = bestmodel.predict(test_features)\n",
    "dirtytest_probabilities = bestmodel.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "placed-ranch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.815028901734104"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dirtytest_predictions == dirtytest['authgender']) / len(dirtytest['authgender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-heritage",
   "metadata": {},
   "source": [
    "So, looking both at the cross-validation and at the held-out test set, it appears that we get *slightly* worse accuracy with dirty data.\n",
    "\n",
    "### The actual fun stuff: what creates the error?\n",
    "\n",
    "I've saved the prediction probabilities from cross-validation on the training set (and from prediction on test), so that we can measure error for each chunk and try to diagnose the sources of error.\n",
    "\n",
    "Our strategy here involves three measurements.\n",
    "\n",
    "1. The \"divergence from clean\": how much does prediction error on a dirty chunk exceed (or fall below) the average error for clean chunks of this volume. Note that we cannot create a chunk-to-chunk mapping, since Hathi has not been trimmed.\n",
    "\n",
    "2. The \"worderrors\" for the chunk: how many words in the Hathi chunk were not found *either* in the Gutenberg text or in a simple dictionary of 10,000 common English words? This is not exactly the same as word-error-rate, but it's a close approximation that we can use for the moment.\n",
    "\n",
    "3. The \"passagefails\" for the chunk. What fraction of 20-character segments in the Hathi chunk could be found with fuzzy matching in (the whole) Gutenberg volume. We accepted difflib similarity scores of > 0.78 as a a match. This is **in effect** a measurement of the amount of text in the chunk that is substantially *different from* the Gutenberg text, because it's a table of contents, or introduction, or notes -- or a different translation!\n",
    "\n",
    "The question we're trying to answer: does accuracy fall mostly because of OCR errors, or because of paratext? As you'll see, it's difficult to answer with these corpora. This evidence casts some light, but we will also need more experiments with multiple corpora.\n",
    "\n",
    "We start by constructing vectors for real y and predicted $\\widehat{y}$ — for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "official-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_y = dirtymodelmeta['authgender'].map({'f': 1, 'm': 0})  # we already put this in train-test order\n",
    "                                                            # so it will match the next line\n",
    "dirty_probabilities = np.append(dirtytrain_probabilities[ : , 0], dirtytest_probabilities[ : , 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-employer",
   "metadata": {},
   "source": [
    "We can reason about two kinds of error. \n",
    "\n",
    "Directional error: did this chunk appear more negative or positive than it was in the real y vector.\n",
    "\n",
    "Absolute error: how far was this chunk from the real value in either direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "elder-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_error = dirty_y - dirty_probabilities\n",
    "dirty_absolute_error = np.abs(dirty_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-assessment",
   "metadata": {},
   "source": [
    "Repeat the above steps for the clean model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "objective-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_y = cleanmodelmeta['authgender'].map({'f': 1, 'm': 0})\n",
    "\n",
    "clean_probabilities = np.append(cleantrain_probabilities[ : , 0], cleantest_probabilities[ : , 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "polar-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_error = clean_y - clean_probabilities\n",
    "clean_absolute_error = np.abs(clean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-original",
   "metadata": {},
   "source": [
    "#### Create mean values for clean volumes\n",
    "\n",
    "We create a dataframe and then groupby / average on Gutenberg indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "thick-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gbi = [get_gbindex(x) for x in cleanmodelmeta['chunkid']]\n",
    "clean_df = pd.DataFrame({'chunkid': cleanmodelmeta['chunkid'], 'gbindex': clean_gbi,\n",
    "                         'clean_error': clean_error, 'clean_abs_error': clean_absolute_error})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "individual-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "meandf = clean_df.groupby('gbindex').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "detected-fourth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_error</th>\n",
       "      <th>clean_abs_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbindex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48795</th>\n",
       "      <td>-0.380096</td>\n",
       "      <td>0.380096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35557</th>\n",
       "      <td>0.518421</td>\n",
       "      <td>0.518421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57149</th>\n",
       "      <td>0.368115</td>\n",
       "      <td>0.368115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37908</th>\n",
       "      <td>0.217119</td>\n",
       "      <td>0.217119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3841</th>\n",
       "      <td>0.417768</td>\n",
       "      <td>0.417768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RichSPO3fic</th>\n",
       "      <td>-0.381683</td>\n",
       "      <td>0.381683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37361</th>\n",
       "      <td>-0.413712</td>\n",
       "      <td>0.413712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28694</th>\n",
       "      <td>-0.108656</td>\n",
       "      <td>0.108656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35500</th>\n",
       "      <td>-0.213543</td>\n",
       "      <td>0.213543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32144</th>\n",
       "      <td>0.193282</td>\n",
       "      <td>0.193282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             clean_error  clean_abs_error\n",
       "gbindex                                  \n",
       "48795          -0.380096         0.380096\n",
       "35557           0.518421         0.518421\n",
       "57149           0.368115         0.368115\n",
       "37908           0.217119         0.217119\n",
       "3841            0.417768         0.417768\n",
       "RichSPO3fic    -0.381683         0.381683\n",
       "37361          -0.413712         0.413712\n",
       "28694          -0.108656         0.108656\n",
       "35500          -0.213543         0.213543\n",
       "32144           0.193282         0.193282"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meandf.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "surface-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_gbi = [get_gbindex(x) for x in dirtymodelmeta['chunkid']]\n",
    "dirty_df = pd.DataFrame({'chunkid': list(dirtymodelmeta['chunkid']), 'gbindex': dirty_gbi,\n",
    "                         'dirty_error': dirty_error, 'dirty_abs_error': dirty_absolute_error})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "egyptian-champion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunkid</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>dirty_error</th>\n",
       "      <th>dirty_abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>6593_26</td>\n",
       "      <td>6593</td>\n",
       "      <td>-0.253745</td>\n",
       "      <td>0.253745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>696_2</td>\n",
       "      <td>696</td>\n",
       "      <td>-0.800630</td>\n",
       "      <td>0.800630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>51932_3</td>\n",
       "      <td>51932</td>\n",
       "      <td>0.150403</td>\n",
       "      <td>0.150403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>44262_8</td>\n",
       "      <td>44262</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.167118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>37710_1</td>\n",
       "      <td>37710</td>\n",
       "      <td>0.157606</td>\n",
       "      <td>0.157606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      chunkid gbindex  dirty_error  dirty_abs_error\n",
       "459   6593_26    6593    -0.253745         0.253745\n",
       "623     696_2     696    -0.800630         0.800630\n",
       "234   51932_3   51932     0.150403         0.150403\n",
       "1302  44262_8   44262     0.167118         0.167118\n",
       "1353  37710_1   37710     0.157606         0.157606"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "wrong-despite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunkid</th>\n",
       "      <th>gbindex</th>\n",
       "      <th>dirty_error</th>\n",
       "      <th>dirty_abs_error</th>\n",
       "      <th>clean_error</th>\n",
       "      <th>clean_abs_error</th>\n",
       "      <th>error_divergence</th>\n",
       "      <th>divergence_of_abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6098_2</td>\n",
       "      <td>6098</td>\n",
       "      <td>-0.714614</td>\n",
       "      <td>0.714614</td>\n",
       "      <td>-0.605641</td>\n",
       "      <td>0.605641</td>\n",
       "      <td>-0.108973</td>\n",
       "      <td>0.108973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>63339_2</td>\n",
       "      <td>63339</td>\n",
       "      <td>0.725906</td>\n",
       "      <td>0.725906</td>\n",
       "      <td>0.403260</td>\n",
       "      <td>0.403260</td>\n",
       "      <td>0.322646</td>\n",
       "      <td>0.322646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>manley.01_5</td>\n",
       "      <td>manley.01</td>\n",
       "      <td>0.087757</td>\n",
       "      <td>0.087757</td>\n",
       "      <td>0.209378</td>\n",
       "      <td>0.209378</td>\n",
       "      <td>-0.121621</td>\n",
       "      <td>-0.121621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>2512_0</td>\n",
       "      <td>2512</td>\n",
       "      <td>-0.019852</td>\n",
       "      <td>0.019852</td>\n",
       "      <td>-0.094151</td>\n",
       "      <td>0.094151</td>\n",
       "      <td>0.074299</td>\n",
       "      <td>-0.074299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>LewiMMR2fic_3</td>\n",
       "      <td>LewiMMR2fic</td>\n",
       "      <td>-0.902269</td>\n",
       "      <td>0.902269</td>\n",
       "      <td>-0.644809</td>\n",
       "      <td>0.644809</td>\n",
       "      <td>-0.257460</td>\n",
       "      <td>0.257460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>33537_5</td>\n",
       "      <td>33537</td>\n",
       "      <td>0.131193</td>\n",
       "      <td>0.131193</td>\n",
       "      <td>0.292917</td>\n",
       "      <td>0.292917</td>\n",
       "      <td>-0.161724</td>\n",
       "      <td>-0.161724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>55594_6</td>\n",
       "      <td>55594</td>\n",
       "      <td>0.081366</td>\n",
       "      <td>0.081366</td>\n",
       "      <td>0.498117</td>\n",
       "      <td>0.498117</td>\n",
       "      <td>-0.416751</td>\n",
       "      <td>-0.416751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>52019_4</td>\n",
       "      <td>52019</td>\n",
       "      <td>0.019685</td>\n",
       "      <td>0.019685</td>\n",
       "      <td>0.147768</td>\n",
       "      <td>0.147768</td>\n",
       "      <td>-0.128084</td>\n",
       "      <td>-0.128084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>34476_3</td>\n",
       "      <td>34476</td>\n",
       "      <td>-0.417151</td>\n",
       "      <td>0.417151</td>\n",
       "      <td>-0.277048</td>\n",
       "      <td>0.277048</td>\n",
       "      <td>-0.140103</td>\n",
       "      <td>0.140103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>35548_4</td>\n",
       "      <td>35548</td>\n",
       "      <td>0.030277</td>\n",
       "      <td>0.030277</td>\n",
       "      <td>0.222919</td>\n",
       "      <td>0.222919</td>\n",
       "      <td>-0.192642</td>\n",
       "      <td>-0.192642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            chunkid      gbindex  dirty_error  dirty_abs_error  clean_error  \\\n",
       "68           6098_2         6098    -0.714614         0.714614    -0.605641   \n",
       "762         63339_2        63339     0.725906         0.725906     0.403260   \n",
       "1147    manley.01_5    manley.01     0.087757         0.087757     0.209378   \n",
       "976          2512_0         2512    -0.019852         0.019852    -0.094151   \n",
       "1282  LewiMMR2fic_3  LewiMMR2fic    -0.902269         0.902269    -0.644809   \n",
       "1361        33537_5        33537     0.131193         0.131193     0.292917   \n",
       "1545        55594_6        55594     0.081366         0.081366     0.498117   \n",
       "1072        52019_4        52019     0.019685         0.019685     0.147768   \n",
       "1514        34476_3        34476    -0.417151         0.417151    -0.277048   \n",
       "292         35548_4        35548     0.030277         0.030277     0.222919   \n",
       "\n",
       "      clean_abs_error  error_divergence  divergence_of_abs_error  \n",
       "68           0.605641         -0.108973                 0.108973  \n",
       "762          0.403260          0.322646                 0.322646  \n",
       "1147         0.209378         -0.121621                -0.121621  \n",
       "976          0.094151          0.074299                -0.074299  \n",
       "1282         0.644809         -0.257460                 0.257460  \n",
       "1361         0.292917         -0.161724                -0.161724  \n",
       "1545         0.498117         -0.416751                -0.416751  \n",
       "1072         0.147768         -0.128084                -0.128084  \n",
       "1514         0.277048         -0.140103                 0.140103  \n",
       "292          0.222919         -0.192642                -0.192642  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty_df = dirty_df.merge(meandf, on = 'gbindex')\n",
    "dirty_df['error_divergence'] = dirty_df['dirty_error'] - dirty_df['clean_error']\n",
    "dirty_df['divergence_of_abs_error'] = dirty_df['dirty_abs_error'] - dirty_df['clean_abs_error']\n",
    "dirty_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "multiple-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "error0 = pd.read_csv('/Users/tunder/work/gh_align/hathinocorrerrs0.tsv', sep = '\\t')\n",
    "error1 = pd.read_csv('/Users/tunder/work/gh_align/hathinocorrerrs1.tsv', sep = '\\t')\n",
    "error2 = pd.read_csv('/Users/tunder/work/gh_align/hathinocorrerrs2.tsv', sep = '\\t')\n",
    "error3 = pd.read_csv('/Users/tunder/work/gh_align/hathinocorrerrs3.tsv', sep = '\\t')\n",
    "error4 = pd.read_csv('/Users/tunder/work/gh_align/hathinocorrerrs4.tsv', sep = '\\t')\n",
    "#error5 = pd.read_csv('/Users/tunder/work/gh_align/hathichunkerrs5.tsv', sep = '\\t')\n",
    "chunkerrors = pd.concat([error0, error1, error2, error3, error4], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "engaging-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunkerrors.drop_duplicates(inplace = True)\n",
    "#print(chunkerrors.shape)\n",
    "#chunkerrors.to_csv('../metadata/hathi_chunk_errors.tsv', sep = '\\t', index = False)\n",
    "#chunkerrors = pd.read_csv('../metadata/hathi_chunk_errors.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "advance-apartment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chunkid', 'passagefails', 'worderrors'], dtype='object')"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkerrors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "sapphire-hearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2624, 10)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkerrors = chunkerrors.merge(dirty_df, on = 'chunkid', how = 'left')\n",
    "chunkerrors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "exempt-visitor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1648, 10)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkerrors.dropna(inplace=True)\n",
    "chunkerrors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "consecutive-spirit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.04157113940721571, 0.09159321190666851)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['error_divergence'], chunkerrors['divergence_of_abs_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "progressive-petroleum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10461367641032888, 2.0875993389670293e-05)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['passagefails'], chunkerrors['error_divergence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "engaged-adobe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04886063436452093, 0.04734457556816991)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['passagefails'], chunkerrors['divergence_of_abs_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "conceptual-mapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.08972116516622668, 0.0002654491810300403)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['worderrors'], chunkerrors['error_divergence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "suited-jewel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0006329139431856881, 0.9795173562008995)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['worderrors'], chunkerrors['divergence_of_abs_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-penalty",
   "metadata": {},
   "source": [
    "The sheer amount of error correlates *barely,* and *very weakly* with paratext. It has no relation to OCR error.\n",
    "\n",
    "The *direction* of change (```error_divergence```) has a significant relation to both OCR and paratext. Paratext makes the model more likely to interpret a text as written by a woman; OCR error makes it more likely to attribute it to a man.\n",
    "\n",
    "But all of these relationships are fairly weak.\n",
    "\n",
    "We can also reason about the absolute value of the difference between clean and dirty. In other words, we could decide not to care whether error was increased or decreased, or which direction the prediction moved, and instead care only about how *different* the prediction is. Here only paratext matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "yellow-gateway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10027685258057445, 4.5426894304919116e-05)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['passagefails'], np.abs(chunkerrors['error_divergence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "logical-crowd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.010698034598122123, 0.66430611153246)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['worderrors'], np.abs(chunkerrors['error_divergence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-sally",
   "metadata": {},
   "source": [
    "That won't change if we substitute ```divergence_of_abs_error```.\n",
    "\n",
    "Finally, we want to think about the correlation of our predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "alleged-technology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06263192647539553, 0.010985920807017623)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(chunkerrors['passagefails'], chunkerrors['worderrors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-teddy",
   "metadata": {},
   "source": [
    "**NOTE**: That's not a high correlation. There's not a huge danger of collinearity now.\n",
    "\n",
    "## Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "negative-passing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               OLS Regression Results                              \n",
      "===================================================================================\n",
      "Dep. Variable:     divergence_of_abs_error   R-squared:                       0.002\n",
      "Model:                                 OLS   Adj. R-squared:                  0.001\n",
      "Method:                      Least Squares   F-statistic:                     1.973\n",
      "Date:                     Thu, 29 Apr 2021   Prob (F-statistic):              0.139\n",
      "Time:                             08:11:37   Log-Likelihood:                 359.26\n",
      "No. Observations:                     1648   AIC:                            -712.5\n",
      "Df Residuals:                         1645   BIC:                            -696.3\n",
      "Df Model:                                2                                         \n",
      "Covariance Type:                 nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const           -0.0552      0.006     -9.381      0.000      -0.067      -0.044\n",
      "passagefails     0.0458      0.023      1.986      0.047       0.001       0.091\n",
      "worderrors      -0.0099      0.100     -0.099      0.921      -0.207       0.187\n",
      "==============================================================================\n",
      "Omnibus:                      104.319   Durbin-Watson:                   1.599\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              123.059\n",
      "Skew:                           0.658   Prob(JB):                     1.90e-27\n",
      "Kurtosis:                       3.251   Cond. No.                         21.0\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "y = chunkerrors['divergence_of_abs_error']\n",
    "X = sm.add_constant(chunkerrors.loc[ : , ['passagefails', 'worderrors']])\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-cliff",
   "metadata": {},
   "source": [
    "So, in short, when we consider both the gross inclusion of text that's not in Gutenberg (```passagefails```) and OCR quality, as measured by spelling errors in individual words (```worderrors```), only the former appears to be significantly distorting for the absolute amount of error.\n",
    "\n",
    "When we consider change directionally, they're both significantly associated (but are presumably to some extent cancelling each other out). Note that we've established ```passagefails``` and ```worderrors``` aren't significantly associated at the chunk level, but they might still be associated at the level of volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "light-laptop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       error_divergence   R-squared:                       0.020\n",
      "Model:                            OLS   Adj. R-squared:                  0.019\n",
      "Method:                 Least Squares   F-statistic:                     17.00\n",
      "Date:                Thu, 29 Apr 2021   Prob (F-statistic):           4.93e-08\n",
      "Time:                        08:11:50   Log-Likelihood:                 319.01\n",
      "No. Observations:                1648   AIC:                            -632.0\n",
      "Df Residuals:                    1645   BIC:                            -615.8\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const            0.0069      0.006      1.138      0.255      -0.005       0.019\n",
      "passagefails     0.1070      0.024      4.526      0.000       0.061       0.153\n",
      "worderrors      -0.4062      0.103     -3.953      0.000      -0.608      -0.205\n",
      "==============================================================================\n",
      "Omnibus:                       10.846   Durbin-Watson:                   1.514\n",
      "Prob(Omnibus):                  0.004   Jarque-Bera (JB):                8.047\n",
      "Skew:                           0.049   Prob(JB):                       0.0179\n",
      "Kurtosis:                       2.672   Cond. No.                         21.0\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "y = chunkerrors['error_divergence']\n",
    "X = sm.add_constant(chunkerrors.loc[ : , ['passagefails', 'worderrors']])\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-voice",
   "metadata": {},
   "source": [
    "For this problem (gender prediction), none of the effects are really huge. Practically speaking, for downstream conclusions, none of the errors are a big thing to worry about. That may not hold equally true for genre and date prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
